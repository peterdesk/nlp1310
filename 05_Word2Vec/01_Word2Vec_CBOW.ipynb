{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론(예측) 기반 기법과 신경망\n",
    "\n",
    "## word2vec : 워드투벡터\n",
    "2013년 구글의 토마스미콜로프(Tomas Mikolov)의 팀이 개발<br>\n",
    "<b>word2vec</b> 알고리즘은 <b>신경망 모델</b>을 사용 하여 큰 텍스트 코퍼스에서 단어 연관성을 학습. 학습이 끝나면 이러한 모델은 동의어 단어를 감지하거나 부분 문장에 대한 추가 단어를 제안 할 수 있다. word2vec는 <b>벡터</b> 라고하는 특정 숫자 목록을 사용하여 각각의 고유 한 단어를 나타낸다 . 벡터는 간단한 수학적 함수 ( 벡터 간의 코사인 유사성 ) 가 해당 벡터가 나타내는 단어 간의 의미 유사성 수준을 나타내 도록 신중하게 선택 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] 신경망에서의 단어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\n",
      " [[1 0 0 0 0 0 0]]\n",
      "W:\n",
      " [[-1.24934016 -0.3158433  -1.80566262]\n",
      " [ 0.33308404 -0.06382567 -1.82409671]\n",
      " [ 0.26262167 -0.97305858 -0.81414364]\n",
      " [-0.85627881 -0.92024437  0.0423612 ]\n",
      " [ 0.02460474 -1.63968088 -0.0529423 ]\n",
      " [ 0.48160653 -0.26763117 -0.55285465]\n",
      " [-1.29746668 -1.6890333   0.36078534]]\n",
      "h:\n",
      " [[-1.24934016 -0.3158433  -1.80566262]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = 'You say goodbye and I say hello.' \n",
    "# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
    "\n",
    "# 여기서 'you'만  one-hot 인코딩으로 표현\n",
    "c = np.array([[1,0,0,0,0,0,0]])   # (1,7)\n",
    "print('c:\\n',c)\n",
    "\n",
    "W = np.random.randn(7,3)          # (7,3)\n",
    "print('W:\\n',W)\n",
    "\n",
    "h = np.matmul(c,W)    # (1,7) * (7,3) = (1,3)\n",
    "print('h:\\n',h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self,W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x,W)\n",
    "        self.x = x\n",
    "        return out\n",
    "        \n",
    "    def backward(self,dout):\n",
    "        W = self.params\n",
    "        dx = np.dot(dout,W.T)\n",
    "        dw = np.dot(self.x.T,dout)\n",
    "        self.grads[0][...] = dw  # 깊은복사\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\n",
      " [[1 0 0 0 0 0 0]]\n",
      "W:\n",
      " [[ 0.03809138  0.41114613  1.19362157]\n",
      " [ 0.283477   -0.67532005  0.65685622]\n",
      " [ 0.53342999  1.14969612 -0.39897379]\n",
      " [-0.58489113 -1.39395341 -0.50134693]\n",
      " [ 1.24099499  1.26852592  0.19266246]\n",
      " [ 0.9430991   1.56338641 -0.62183316]\n",
      " [-0.5466878   0.2754619  -0.56581318]]\n",
      "h:\n",
      " [[0.03809138 0.41114613 1.19362157]]\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.' \n",
    "# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
    "\n",
    "# 여기서 'you'만  one-hot 인코딩으로 표현\n",
    "c = np.array([[1,0,0,0,0,0,0]])   # (1,7)\n",
    "print('c:\\n',c)\n",
    "\n",
    "W = np.random.randn(7,3)          # (7,3)\n",
    "print('W:\\n',W)\n",
    "\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print('h:\\n',h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 단순한 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW (Continuous Bag of Words) 모델\n",
    "\n",
    "#### Word2Vec에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있다\n",
    "- $ CBOW $ 는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법 <br>\n",
    "  타깃(target)은 중앙 단어 그 주변 단어들이 맥락(contexts)이다\n",
    "- $ Skip-Gram $ 은 중간에 있는 단어로 주변 단어들을 예측하는 방법\n",
    "\n",
    "#### BOW(Bag of Words) : 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "\n",
    "BOW를 만드는 과정<br>\n",
    "(1) 우선, 각 단어의 고유한 인덱스(Index)를 부여한다.<br>\n",
    "(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터(Vector)를 만든다.<br>\n",
    "\n",
    "\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"<br>\n",
    "('정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9) <br>\n",
    "BOW: [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]  ==> '가' 와 '물가상승률' 은 2회 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46504032 0.19727523 1.81107858]] (1, 3)\n",
      "[[-0.77922543  2.26429649  1.67264438 -3.37697764  2.35950193 -2.41414434\n",
      "   3.27086511]] (1, 7)\n"
     ]
    }
   ],
   "source": [
    "# (CBOW 전체구조의 Preview)\n",
    "# 샘플 맥락 데이터 : 2개의 주변 단어를 맥락으로 중간 단어('say')를 예측\n",
    "\n",
    "text = 'You say goodbye and I say hello.' \n",
    "\n",
    "# 2개의 주변 단어를 one-hot 벡터 생성\n",
    "c0 = np.array([[1,0,0,0,0,0,0]])  # 'you' , (1,7)\n",
    "c1 = np.array([[0,0,1,0,0,0,0]])  # 'goodbye' , (1,7)\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7,3)\n",
    "W_out = np.random.randn(3,7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0) # (1,7) * (7,3) = (1,3)\n",
    "h1 = in_layer1.forward(c1) # (1,7) * (7,3) = (1,3)\n",
    "h = 0.5*(h0 + h1)\n",
    "print(h,h.shape)\n",
    "s = out_layer.forward(h)   # (1,3) * (3,7) = (1,7)\n",
    "print(s,s.shape)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 학습 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynlp import preprocess\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.' \n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)      # 8 개\n",
    "print(id_to_word)  # 7 개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: [1 2 3 4 1 5]\n",
      "contexts:\n",
      " [[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n"
     ]
    }
   ],
   "source": [
    "# target : (6,)\n",
    "target = corpus[1:-1]  # 타깃(중간단어) : [1 2 3 4 1 5], 첫번째와 마지막 단어 제외\n",
    "print('target:',target)\n",
    "\n",
    "contexts = []\n",
    "for idx in range(1,len(corpus) - 1 ): # 1 to 6,6회 반복 , 중간단어마다 앞뒤 주변단어 조합 6가지\n",
    "    cs = []\n",
    "    for t in range(-1,2): # 3회, -1,0,1\n",
    "        if t == 0:\n",
    "            continue\n",
    "        cs.append(corpus[idx + t]) \n",
    "    contexts.append(cs)  \n",
    "\n",
    "print('contexts:\\n',np.array(contexts))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts와 target을 구하는 함수\n",
    "def create_contexts_target(corpus,window_size=1):\n",
    "    target = corpus[window_size:-window_size]  \n",
    "\n",
    "    contexts = []\n",
    "    for idx in(range(window_size,len(corpus) -window_size)): \n",
    "        cs = []\n",
    "        for t in range(-window_size,window_size+1) : \n",
    "            if t == 0: \n",
    "                continue\n",
    "            cs.append(corpus[idx + t]) \n",
    "        contexts.append(cs)\n",
    "    return np.array(contexts),np.array(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n"
     ]
    }
   ],
   "source": [
    "contexts,target = create_contexts_target(corpus,window_size=1)\n",
    "print(contexts)  # (6,2)\n",
    "\n",
    "# 맥락(contexts) : 예측할 단어의 주변 단어\n",
    "# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
    "# window_size=1 일 경우 : 주변 단어를 중간 단어에 앞,뒤로 1개만 사용\n",
    "# [[0 2]   : 'you', 'goodbye'\n",
    "#  [1 3]   : 'say', 'and'\n",
    "#  [2 4]   : 'goodbye', 'i'\n",
    "#  [3 1]   : 'and', 'say'\n",
    "#  [4 5]   : 'i', 'hello'\n",
    "#  [1 6]]  : 'say', '.'\n",
    "\n",
    "# window_size=2 일 경우 : : 주변 단어를 중간 단어에 앞,뒤로 2개 사용\n",
    "# [[0 1 3 4]\n",
    "#  [1 2 4 1]\n",
    "#  [2 3 1 5]\n",
    "#  [3 4 5 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "print(target)   # (6,)\n",
    "# 타깃(target) : 예측할 단어, 중간단어, 6개\n",
    "# ['say','goodbye','and','i','say','hello']\n",
    "# [1 2 3 4 1 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맥락과 타깃을 원핫 표현으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 변환 함수\n",
    "\n",
    "# target [1 2 3 4 1 5]을 변환하는 경우를 주석으로 설명 \n",
    "def convert_one_hot(corpus, vocab_size):  # [1 2 3 4 1 5], 7\n",
    "    N = corpus.shape[0] # (6,) --> 6\n",
    "\n",
    "    if corpus.ndim == 1: # target [1 2 3 4 1 5], 1차원인경우 ==> 2차원으로 출력\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32) # 0으로 초기화된 (6,7) 2차원 배열 생성 \n",
    "        for idx, word_id in enumerate(corpus): # 6회 반복\n",
    "            one_hot[idx, word_id] = 1  # one_hot[0,1] = 1, [1,2]=1, [2,3] = 1,...,  [3,4],[4,1],[5,5] = 1...\n",
    "\n",
    "    elif corpus.ndim == 2: # contexts 2차원 인경우 ==> 3차원으로 출력\n",
    "        C = corpus.shape[1] # (6,2) --> 2\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32) # 0으로 초기화된 (6,2,7) 3차원 배열 생성 \n",
    "        for idx_0, word_ids in enumerate(corpus): # 6회\n",
    "            for idx_1, word_id in enumerate(word_ids): #  2회\n",
    "                one_hot[idx_0, idx_1, word_id] = 1  \n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "print(vocab_size) # 7\n",
    "\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "(6, 7)\n"
     ]
    }
   ],
   "source": [
    "print(target)\n",
    "print(target.shape)  # (6, 7)\n",
    "# [1 2 3 4 1 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2, 7)\n",
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n"
     ]
    }
   ],
   "source": [
    "print(contexts.shape) # (6, 2, 7)\n",
    "print(contexts)\n",
    "# [[0 2]\n",
    "#  [1 3]\n",
    "#  [2 4]\n",
    "#  [3 1]\n",
    "#  [4 5]\n",
    "#  [1 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] CBOW 신경망 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_layers import MatMul, SoftmaxWithLoss, SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 분류 모델 : Softmax 사용\n",
    "class SimpleCBOW :\n",
    "    def __init__(self, vocab_size, hidden_size): # 단어의 갯수: 7, 은닉층의 뉴런: 5\n",
    "        V,H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01*np.random.randn(V,H).astype('f') # (7,5)\n",
    "        W_out = 0.01*np.random.randn(H,V).astype('f') # (5,7)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in layers : # 3회\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        # 인스턴스 변수 단어의 분산 표현을 저장한다.    \n",
    "        self.word_vec = W_in    \n",
    "        \n",
    "    def predict(self,contexts):  # contexts :  (6, 2, 7)\n",
    "        \n",
    "        # (6,7) * (7,5)  =  (6,5)\n",
    "        h0 = self.in_layer0.forward(contexts[:,0]) #  (6,7)으로 입력, 맥락의 첫번째 단어 \n",
    "        h1 = self.in_layer1.forward(contexts[:,1]) #  (6,7)으로 입력, 맥락의 두번째 단어\n",
    "        \n",
    "        h = (h0 + h1) * 0.5  # 평균\n",
    "        \n",
    "        # (6,5) * (5,7) = (6,7)\n",
    "        score = self.out_layer.forward(h)\n",
    "        return self.loss_layer.softmax(score) # softmax()함수로 확률값으로 출력\n",
    "    \n",
    "    def forward(self,contexts,target):  # contexts :  (6, 2, 7), target : (6,7)\n",
    "        \n",
    "        # (6,7) * (7,5)  =  (6,5)\n",
    "        h0 = self.in_layer0.forward(contexts[:,0]) #  (6,7)으로 입력, 맥락의 첫번째 단어 \n",
    "        h1 = self.in_layer1.forward(contexts[:,1]) #  (6,7)으로 입력, 맥락의 두번째 단어\n",
    "        \n",
    "        h = (h0 + h1) * 0.5  # 평균\n",
    "        \n",
    "        # (6,5) * (5,7) = (6,7)\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score,target)        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        \n",
    "        self.in_layer0.backward(da)\n",
    "        self.in_layer1.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수의 중복 제거 함수\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시  : lSTM에서 사용\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                \n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
