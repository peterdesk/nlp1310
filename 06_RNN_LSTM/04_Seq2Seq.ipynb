{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq : Sequence to Sequence 모델\n",
    "#### : Encoder Decoder  모델이라고도 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "class Encoder:\n",
    "    def __init__(self,vocab_size, wordvec_size,hidden_size ):\n",
    "        V,D,H = vocab_size, wordvec_size,hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V,D) / 100).astype('f')       \n",
    "        lstm_Wx = (rn(D,4*H) / np.sqrt(D)).astype('f') \n",
    "        lstm_Wh = (rn(H,4*H) / np.sqrt(H)).astype('f') \n",
    "        lstm_b = np.zeros(4*H).astype('f')             \n",
    "        \n",
    "        # 계층 생성\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=False)\n",
    "        \n",
    "        # 모든 가중치와 기울기를 모은다.\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "\n",
    "    def forward(self,xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)    # [N,T,H] , 3차원\n",
    "        self.hs = hs\n",
    "        return hs[:,-1,:]             # TimeLSTM 계층의 마지막 은닉 상태 h를 반환, [N,H], 2차원\n",
    "    \n",
    "    def backward(self,dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:,-1,:] = dh\n",
    "        \n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder class : TimeSoftmaxWithLoss는 계층에 생성하지 않는다\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size ):\n",
    "        V,D,H = vocab_size, wordvec_size,hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V,D) / 100).astype('f')       \n",
    "        lstm_Wx = (rn(D,4*H) / np.sqrt(D)).astype('f') \n",
    "        lstm_Wh = (rn(H,4*H) / np.sqrt(H)).astype('f') \n",
    "        lstm_b = np.zeros(4*H).astype('f')  \n",
    "\n",
    "        affine_W = (rn(H,V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        \n",
    "        # 계층 생성\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=False)\n",
    "        self.affine = TimeAffine(affine_W,affine_b)\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params,self.grads = [],[]\n",
    "        for layer in (self.embed,self.lstm,self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self,xs,h):           # h : Encoder의 출력, (N,H)\n",
    "        self.lstm.set_state(h)     \n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out) \n",
    "        score = self.affine.forward(out)\n",
    "        return score     # softmax를 통과시키지 않고 그냥 출력\n",
    "    \n",
    "    def backward(self,dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        \n",
    "        dh = self.lstm.dh     # TimeLSTM의 backward()에서 dh가 얻어져 저장 되어 있으므로\n",
    "        \n",
    "        return dh             # Encoder 에 전달 \n",
    "    \n",
    "     # 문장 생성시 호출   \n",
    "    def generate(self,h,start_id,sample_size) :\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape(1,1)\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out) \n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            sample_id = np.argmax(score.flatten()) # 점수가 가장 큰 문자의 ID를 선택,결정적 방법\n",
    "            sampled.append(int(sample_id))\n",
    "            \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V,D,H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        self.encoder = Encoder(V,D,H)\n",
    "        self.decoder = Decoder(V,D,H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "        \n",
    "    def forward(self,xs,ts):\n",
    "        decoder_xs, decoder_ts = ts[:,:-1], ts[:,1:] # decoder의 입력: 마지막 단어를 제외\n",
    "                                                     # softmaxwithloss 의 입력 : 첫 단어를 제외\n",
    "        \n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs,h)\n",
    "        loss = self.softmax.forward(score,decoder_ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "    \n",
    "    def generate(self,xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h,start_id,sample_size)\n",
    "        \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 준비\n",
    "### Toy Data Set : 'addition.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '6': 1, '+': 2, '7': 3, '5': 4, ' ': 5, '_': 6, '9': 7, '2': 8, '0': 9, '3': 10, '8': 11, '4': 12}\n",
      "{0: '1', 1: '6', 2: '+', 3: '7', 4: '5', 5: ' ', 6: '_', 7: '9', 8: '2', 9: '0', 10: '3', 11: '8', 12: '4'}\n",
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "[ 3  0  2  0  0 11  5]\n",
      "[ 6  0 11  7  5]\n",
      "71+118 \n",
      "_189 \n",
      "510+223\n",
      "_733 \n"
     ]
    }
   ],
   "source": [
    "from dataset import sequence\n",
    "\n",
    "# 'addition.txt': 총 50000개 덧셈 연산 예를 가짐\n",
    "\n",
    "# 덧셈식을 일반 문장과 같이 corpus를 생성하고 seed가 고정된 랜덤으로 뒤섞고 90:10 비율로 학습과 검증 데이터를 분리해준다\n",
    "# x는 덧셈식, t는 덧셈 결과값\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = sequence.load_data('addition.txt', seed = 1984)\n",
    "\n",
    "char_to_id ,id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(char_to_id)\n",
    "print(id_to_char)\n",
    "# 총 13개의 문자를 value로 갖음: '0','1','2','3','4','5','6','7','8','9','+',' ','_'\n",
    "\n",
    "print(x_train.shape,t_train.shape)  # (45000, 7) (45000, 5)\n",
    "print(x_test.shape,t_test.shape)    # (5000, 7) (5000, 5)\n",
    "\n",
    "print(x_train.shape,t_train.shape)  # (45000, 7) (45000, 5)\n",
    "print(x_test.shape,t_test.shape)    # (5000, 7) (5000, 5)\n",
    "\n",
    "print(x_train[0])\n",
    "print(t_train[0])\n",
    "\n",
    "print(''.join(id_to_char[c] for c in x_train[0])) # 71+118 \n",
    "print(''.join(id_to_char[c] for c in t_train[0])) # _189\n",
    "\n",
    "print(''.join(id_to_char[c] for c in x_train[1]))   \n",
    "print(''.join(id_to_char[c] for c in t_train[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def eval_seq2seq(model, question, correct, id_to_char,\n",
    "                 verbos=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    # 머릿글자\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model.generate(question, start_id, len(correct))\n",
    "\n",
    "    # 문자열로 변환\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbos:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}  # https://norux.me/29 , 이스케이프 문자\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "\n",
    "        is_windows = os.name == 'nt'\n",
    "\n",
    "        if correct == guess:\n",
    "            mark = colors['ok'] + '☑' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'O'\n",
    "            print(mark + ' ' + guess)\n",
    "        else:\n",
    "            mark = colors['fail'] + '☒' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'X'\n",
    "            print(mark + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 2[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 3[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 5[s] | 손실 2.14\n"
     ]
    }
   ],
   "source": [
    "from nn_layers import Adam,Trainer,TimeEmbedding,TimeLSTM,TimeAffine,TimeSoftmaxWithLoss\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "import numpy as np\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = sequence.load_data('addition.txt', seed = 1984)\n",
    "char_to_id ,id_to_char = sequence.get_vocab()\n",
    "\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "vocab_size = len(char_to_id)  # 13개\n",
    "wordvec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "\n",
    "model = Seq2seq(vocab_size,wordvec_size,hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(max_epoch): # 25회\n",
    "    trainer.fit(x_train,t_train,max_epoch=1,\n",
    "                batch_size=batch_size,max_grad=max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10  # 최초 10개만 맞았는지 틀렸는지  출력\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('검증 정확도 %.3f%%' % (acc * 100))\n",
    "    \n",
    "# 약 6~7분 소요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
