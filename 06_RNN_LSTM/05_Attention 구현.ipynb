{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션(Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맥락 벡터 : 단어 벡터에 가중치를  곱하여 합한 가중합을 구한 벡터\n",
    "단어를 선택하는 작업은 미분 불가하므로 모든 것을 선택하고 단어의 중요도를 가중치로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs:\n",
      " [[-1.74976547  0.3426804   1.1530358  -0.25243604]\n",
      " [ 0.98132079  0.51421884  0.22117967 -1.07004333]\n",
      " [-0.18949583  0.25500144 -0.45802699  0.43516349]\n",
      " [-0.58359505  0.81684707  0.67272081 -0.10441114]\n",
      " [-0.53128038  1.02973269 -0.43813562 -1.11831825]]\n",
      "hs0:\n",
      " [-1.74976547  0.3426804   1.1530358  -0.25243604]\n",
      "a:\n",
      " [0.8  0.1  0.03 0.05 0.02]\n",
      "ar:\n",
      " [[0.8  0.8  0.8  0.8 ]\n",
      " [0.1  0.1  0.1  0.1 ]\n",
      " [0.03 0.03 0.03 0.03]\n",
      " [0.05 0.05 0.05 0.05]\n",
      " [0.02 0.02 0.02 0.02]]\n",
      "t:\n",
      " [[-1.39981238  0.27414432  0.92242864 -0.20194883]\n",
      " [ 0.09813208  0.05142188  0.02211797 -0.10700433]\n",
      " [-0.00568487  0.00765004 -0.01374081  0.0130549 ]\n",
      " [-0.02917975  0.04084235  0.03363604 -0.00522056]\n",
      " [-0.01062561  0.02059465 -0.00876271 -0.02236636]]\n",
      "(5, 4)\n",
      "c  : [-1.34717053  0.39465326  0.95567913 -0.32348518]\n",
      "hs0: [-1.74976547  0.3426804   1.1530358  -0.25243604]\n",
      "(4,)\n",
      "------------------------------------------------------------\n",
      "ar:\n",
      " [[0.8 ]\n",
      " [0.1 ]\n",
      " [0.03]\n",
      " [0.05]\n",
      " [0.02]]\n",
      "t:\n",
      " [[-1.39981238  0.27414432  0.92242864 -0.20194883]\n",
      " [ 0.09813208  0.05142188  0.02211797 -0.10700433]\n",
      " [-0.00568487  0.00765004 -0.01374081  0.0130549 ]\n",
      " [-0.02917975  0.04084235  0.03363604 -0.00522056]\n",
      " [-0.01062561  0.02059465 -0.00876271 -0.02236636]]\n",
      "(5, 4)\n",
      "c  : [-1.34717053  0.39465326  0.95567913 -0.32348518]\n",
      "hs0: [-1.74976547  0.3426804   1.1530358  -0.25243604]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "\n",
    "T, H = 5, 4                # T : 시계열의 길이, H : Hidden size\n",
    "hs = np.random.randn(T,H)  # (5,4)\n",
    "\n",
    "print('hs:\\n',hs)\n",
    "print('hs0:\\n',hs[0])\n",
    "\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02]) # 가중치 , (5,)\n",
    "print('a:\\n',a)\n",
    "\n",
    "# (1) repeat() 함수 사용\n",
    "ar = a.reshape(T,1).repeat(4,axis=1) # (5,1)로 2차원으로 shape을 바꾸고 수평 방향으로 4번 반복 복사\n",
    "print('ar:\\n',ar)                    # (5,4)\n",
    "\n",
    "t = hs * ar                          # (5,4) * (5,4) : 요소간의 곱셈, 단어벡터에 가중치를 곱함\n",
    "print('t:\\n',t)\n",
    "print(t.shape)                       # (5,4)\n",
    "\n",
    "# 가중합\n",
    "c = np.sum(t,axis=0)                 # 수직 방향으로 합 , 가중합, 맥락 벡터\n",
    "print('c  :',c)\n",
    "print('hs0:',hs[0])\n",
    "print(c.shape)                       # (4,)\n",
    "\n",
    "print('-'*60)\n",
    "\n",
    "# (2) 브로드캐스팅 사용, 1번과 결과 동일 \n",
    "ar = a.reshape(T,1)                  # (5,1)로 2차원으로 shape을 바꿈\n",
    "print('ar:\\n',ar)                    # (5,1)\n",
    "\n",
    "t = hs * ar                          # (5,4) * (5,1) : 브로드캐스팅 적용, , 단어벡터에 가중치를 곱함\n",
    "print('t:\\n',t)\n",
    "print(t.shape)                       # (5,4)           \n",
    "\n",
    "# 가중합\n",
    "c = np.sum(t,axis=0)\n",
    "print('c  :',c)\n",
    "print('hs0:',hs[0])\n",
    "print(c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[-1.74976547  0.3426804   1.1530358  -0.25243604  0.98132079]\n",
      " [ 0.51421884  0.22117967 -1.07004333 -0.18949583  0.25500144]\n",
      " [-0.45802699  0.43516349 -0.58359505  0.81684707  0.67272081]\n",
      " [-0.10441114 -0.53128038  1.02973269 -0.43813562 -1.11831825]\n",
      " [ 1.61898166  1.54160517 -0.25187914 -0.84243574  0.18451869]\n",
      " [ 0.9370822   0.73100034  1.36155613 -0.32623806  0.05567601]\n",
      " [ 0.22239961 -1.443217   -0.75635231  0.81645401  0.75044476]\n",
      " [-0.45594693  1.18962227 -1.69061683 -1.35639905 -1.23243451]\n",
      " [-0.54443916 -0.66817174  0.00731456 -0.61293874  1.29974807]\n",
      " [-1.73309562 -0.9833101   0.35750775 -1.6135785   1.47071387]]\n",
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# 3차원 배열의 가중합의 구현, 맥락 벡터\n",
    "np.random.seed(100)\n",
    "\n",
    "N,T,H = 10,5,4             # 면,행,열\n",
    "hs = np.random.randn(N,T,H)\n",
    "# print('hs:\\n',hs)\n",
    "\n",
    "np.random.seed(100)\n",
    "a = np.random.randn(N,T)   # 가중치의 합이 1이 아닌 예임\n",
    "print('a:\\n',a)\n",
    "\n",
    "ar = a.reshape(N,T,1).repeat(H,axis=2)  # 2번축(열)로 4번 반복  , repeat()함수는 생략 가능\n",
    "# print('ar:\\n',ar)\n",
    "\n",
    "t = hs * ar    # (10,5,4)\n",
    "print(t.shape)\n",
    "# print('t:\\n',t)\n",
    "\n",
    "c = np.sum(t, axis=1)   # 1번 축(행)으로 합 , (10,4)\n",
    "print(c.shape)\n",
    "# print('c:\\n',c)\n",
    "# print('hs0:\\n',hs[:,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중합 WeightSum 계층 구현 : 맥락벡터를 구하는 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중합을 구하는 class\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [],[]   # 학습 매개변수가 없는 계층\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N,T,1)#.repeat(T,axis=2)  # repeat노드이지만 repeat 사용하지 않아도 아래 연산시 브로드캐스팅 적용\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis = 1)\n",
    "        \n",
    "        self.cache = (hs,ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self,dc):\n",
    "        hs,ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N,1,H).repeat(T,axis=1)  # sum의 역전파, 출력: (N,T,H)\n",
    "        dar = dt * hs            # (N,T,H)      \n",
    "        dhs = dt * ar            # (N,T,H)\n",
    "        da = np.sum(dar,axis=2)  # repeat의 역전파, 출력 :(N,T) \n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치를 구하는 AttentionWeight 계층 구현\n",
    ": 각 단어의 가중치를 구하여 WeightSum 계층으로 전달한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 가중치 a를 softmax를 사용하여 구하기\n",
    "from nn_layers import softmax, Softmax\n",
    "\n",
    "# nn_layers.py에 아래 Softmax class추가\n",
    "# class Softmax:\n",
    "#     def __init__(self):\n",
    "#         self.params, self.grads = [], []\n",
    "#         self.out = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.out = softmax(x)\n",
    "#         return self.out\n",
    "\n",
    "#     def backward(self, dout):\n",
    "#         dx = self.out * dout\n",
    "#         sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "#         dx -= self.out * sumdx\n",
    "#         return dx\n",
    "\n",
    "N,T,H = 10,5,4\n",
    "hs = np.random.randn(N,T,H)\n",
    "h  = np.random.randn(N,H)\n",
    "\n",
    "hr = h.reshape(N,1,H).repeat(T,axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)        # (N,T,H)\n",
    "\n",
    "s = np.sum(t,axis=2)  # (N,T)\n",
    "print(s.shape)\n",
    "\n",
    "soft_max = Softmax()\n",
    "a = soft_max.forward(s)\n",
    "print(a.shape)\n",
    "print(a.sum(axis=1))\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어의 가중치를 구하는 class\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [],[]   # 학습 매개변수가 없는 계층\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N,1,H)#.repeat(T,axis=1)  # repeat를 사용하지 않아도 아래 곱셈에서 브로드캐스팅으로 repeat\n",
    "        \n",
    "        t = hs * hr                 # (N,T,H)\n",
    "        s = np.sum(t, axis=2)       # (N,T)\n",
    "        a = self.softmax.forward(s) # (N,T)\n",
    "        \n",
    "        self.cache = (hs,hr)\n",
    "        return a                     \n",
    "    \n",
    "    def backward(self,da):\n",
    "        hs,hr = self.cache\n",
    "        N,T,H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)           # (N,T)\n",
    "        dt = ds.reshape(N,T,1).repeat(H,axis=2)  # (N,T,H)\n",
    "        \n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs            # (N,T,H)    \n",
    "        \n",
    "        dh = np.sum(dhr, axis=1) # (N,H)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:  # AttentionWeight과 WeightSum으로 구성\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [],[]   \n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self,hs,h):\n",
    "        a = self.attention_weight_layer.forward(hs,h)\n",
    "        out = self.weight_sum_layer.forward(hs,a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        \n",
    "        dhs = dhs0 + dhs1\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다수의 Attention 계층으로 구성\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [],[]   \n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec) : # hs_enc : Encoder의 출력값, hs_dec:하위 LSTM 출력값 이 입력된다\n",
    "        N,T,H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)   # (N,T,H)\n",
    "        \n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:,t,:] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight) # Attention 계층의 각 단어의 가중치를 리스트에 저장\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        N,T,H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]   # Attention계층\n",
    "            dhs,dh = layer.backward(dout[:,t,:]) # dhs는 Encoder에 전달, dh는 LSTM계층으로 전달\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "            \n",
    "        return dhs_enc, dhs_dec  # dhs_enc는 Encoder에 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_layers import Encoder, Seq2seq\n",
    "# class Encoder:\n",
    "#      ...\n",
    "#     def forward(self,xs):\n",
    "#         xs = self.embed.forward(xs)\n",
    "#         hs = self.lstm.forward(xs)    # [N,T,H] , 3차원\n",
    "#         self.hs = hs\n",
    "#         return hs[:,-1,:]             # TimeLSTM 계층의 마지막 은닉 상태 h를 반환, [N,H], 2차원\n",
    "    \n",
    "#     def backward(self,dh):\n",
    "#         dhs = np.zeros_like(self.hs)\n",
    "#         dhs[:,-1,:] = dh\n",
    "        \n",
    "#         dout = self.lstm.backward(dhs)\n",
    "#         dout = self.embed.backward(dout)\n",
    "#         return dout   \n",
    "\n",
    "class AttentionEncoder(Encoder):   # Encoder를 상속 받아서 구현\n",
    "    def forward(self,xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)    \n",
    "        self.hs = hs\n",
    "        return hs   # Encoder가 마지막 값(hs[:,-1,:])만 반환하는것과 달리 모든 hs를 반환한다, (N,T,H)         \n",
    "    \n",
    "    def backward(self,dhs):\n",
    "        dout = self.lstm.backward(dhs)  # 전체 dhs를 그대로 사용  , (N,T,H)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size ):\n",
    "        V,D,H = vocab_size, wordvec_size,hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V,D) / 100).astype('f')       \n",
    "        lstm_Wx = (rn(D,4*H) / np.sqrt(D)).astype('f') \n",
    "        lstm_Wh = (rn(H,4*H) / np.sqrt(H)).astype('f') \n",
    "        lstm_b = np.zeros(4*H).astype('f')  \n",
    "\n",
    "        affine_W = (rn(2*H,V) / np.sqrt(2*H)).astype('f')  #  LSTM출력과 Attention출력을 concatenate하여 입력됨\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        \n",
    "        # 계층 생성\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=True)  \n",
    "        self.attention = TimeAttention()      # 추가된 부분\n",
    "        self.affine = TimeAffine(affine_W,affine_b)\n",
    "        \n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params,self.grads = [],[]\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self,xs,enc_hs):        \n",
    "        h = enc_hs[:,-1,:]  # Encoder출력의 마지막 값\n",
    "        self.lstm.set_state(h)  \n",
    "        \n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out) \n",
    "        \n",
    "        c = self.attention.forward(enc_hs,dec_hs) # 추가 구현 코드\n",
    "        out = np.concatenate((c,dec_hs),axis=2) # TimeAttention계층의 출력과 LSTM계층의 출력을 합쳐서 Affine계층에 입력\n",
    "                                                # (N,T,2*H)\n",
    "        score = self.affine.forward(out)\n",
    "        return score     \n",
    "    \n",
    "    def backward(self,dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N,T,H2 = dout.shape    # (N,T,2*H)\n",
    "        H = H2//2  # Affine계층 입력시에 concatenate으로 합쳤기 때문에 반으로 나눈다\n",
    "        \n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:] # concatenate의 역전파\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1 # Affine의 역전파 출력과 Attention 역전파의 출력을 합하여 LSTM에 전달\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:,-1,:] += dh         # Encoder의 마지막줄 + dh\n",
    "        dout = self.embed.backward(dout)\n",
    "        \n",
    "        return denc_hs                # Encoder 에 전달 \n",
    "    \n",
    "     # 문장 생성시 호출   \n",
    "    def generate(self,enc_hs,start_id,sample_size) :\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:,-1,:]     # Encoder의 마지막줄\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape(1,1)\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)  # LSTM출력\n",
    "            c = self.attention.forward(enc_hs,dec_hs) \n",
    "            out = np.concatenate((c,dec_hs),axis=2) \n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "            \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # V,D,H = vocab_size, wordvec_size, hidden_size\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        self.encoder = AttentionEncoder(*args)   # AttentionEncoder사용\n",
    "        self.decoder = AttentionDecoder(*args)   # AttentionDecoder사용\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 29) (45000, 11)\n",
      "(5000, 29) (5000, 11)\n",
      "[ 8 22  9 22  9  8  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7]\n",
      "[14 11 12  9  8 15 16  8 15 16  9]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from nn_layers import Softmax, Adam ,Trainer ,TimeEmbedding ,TimeSoftmaxWithLoss,TimeLSTM,TimeAffine\n",
    "# from nn_layers import eval_seq2seq,Seq2seq ,PeekySeq2seq\n",
    "\n",
    "# date.txt : 날짜 형식 변환 데이터, 5만개의 날짜 변환 학습 데이터\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)\n",
    "print(x_train[0])\n",
    "print(t_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 22[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 43[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 63[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 82[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 102[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 123[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 142[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 162[s] | 손실 1.06\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "# max_epoch = 1\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)  \n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 약 1시간 소요\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))   # 정확도 100.000%, Attention이 가장 유리하다\n",
    "\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  어텐션 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from dataset import sequence\n",
    "# from nn_layers import Softmax, Adam ,Trainer ,TimeEmbedding ,TimeSoftmaxWithLoss,TimeLSTM,TimeAffine\n",
    "# from nn_layers import eval_seq2seq,Seq2seq ,PeekySeq2seq\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "# print(len(model.params))\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "    print(t)\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
