{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 사용 문장 생성 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_layers import softmax,TimeDropout,Rnnlm,BetterRnnlm,RnnlmTrainer\n",
    "import numpy as np\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmGen(Rnnlm): # Rnnlm class를 상속 받아 사용\n",
    "    def generate(self,start_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "        word_ids = [start_id]             # start_id : 최초로 시작할 단어\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)       # 3차원\n",
    "            p = softmax(score.flatten())  # 10000개의 단어의 각각의 확률을 구함\n",
    "            # print(p.shape)    # (10000,)\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p)\n",
    "            # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x)) # word_ids 리스트에 샘플링된 단어를 추가\n",
    "        return word_ids\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.lstm_layer.h, self.lstm_layer.c\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.lstm_layer.set_state(*state)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장생성을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you right foreigners intellectual purchased departure absorb merged longstanding goes approved sharon genuine brain anybody previous raider suspend marketers quantities black detergent democratic pushing nixon obtaining province classical rises collecting fcc jurisdiction entire normally depressed burke appetite permitted scottish unwelcome volatility humans fired greenwich beaten unwilling industry reveals worse katz burdens diversification troubling ivan del detailing kick crisis sounded such experiments medication withdrawal successful peaked rake fans compelling considered although eggs expects alike sectors violated ancient pickers climate turmoil proving sticking communication conceded games creation southmark sen shape dozens rich forecasts quarterly cattle omaha whites scientists be easily ltd. spending\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('Rnnlm.pkl')  # 미리 학습된 parameter를 읽어오기\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words =['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100) \n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')  # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)  # 실행시 마다 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 좋은 문장으로 : 2층 LSTM,  Dropout, 가중치 공유 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterRnnlmGen(BetterRnnlm): # BetterRnnlm class를 상속 받아 사용\n",
    "    def generate(self,start_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "        word_ids = [start_id]             # start_id : 최초로 시작할 단어\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)       # 3차원\n",
    "            p = softmax(score.flatten())  # 10000개의 단어의 각각의 확률을 구함\n",
    "            # print(p.shape)    # (10000,)\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p)\n",
    "            # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x)) # word_ids 리스트에 샘플링된 단어를 추가\n",
    "        return word_ids\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for layer in self.lstm_layers:\n",
    "            states.append((layer.h, layer.c))\n",
    "        return states\n",
    "\n",
    "    def set_state(self, states):\n",
    "        for layer, state in zip(self.lstm_layers, states):\n",
    "            layer.set_state(*state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you philadelphia discrepancies toxic benefiting computerized conspiracy businessman alaskan holding price-earnings withdrawal manufacturers wedd host relocation bankers boeing sources who room toyota example translated corning financial auctioned climate districts marriage engines edward soldiers advocates desperate williams touched keen profitability recommend breeding provoked peasants large speaker bicycle hell 40-year-old series completely suitor widens pipes exhausted instrumentation dassault cruise del nervously unicorp upper odeon actively ton colonial bunny duke subscription carson mikhail palace existence receipts corners my ab physician acres coups kong implied owen floating driving coniston drinks cut fiercely noble books splitting translation must imbalances sandinistas listed stimulate figuring bradford premises\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('BetterRnnlm.pkl')  # 미리 학습된 parameter를 읽어오기\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words =['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100) \n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')  # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)  # 실행시 마다 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어열을 초기 값으로 주고 문장을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 4748, 42, 2262, 40]\n",
      "the meaning of life is alfred interested hhs controlling washington-based dismissal thornburgh air carnival decliners wis. nothing successes past satisfied lowest evaluating advice frank threatened voluntarily hesitate courter sums startling indication decent interbank profit publicly acts cry approve threatens cabernet positioned skin calling painful detail covert lionel alike according commentary alleviate attributes glossy write-offs playoffs text financial-services activity notably pepsico threats van informal minimal bitter fueling headquarters misstated bacterium sharp frustration projecting morally examination buffet painting expecting constituents sydney shannon wcrs resorts quiet donoghue brisk officially vicious diet generic ways struck ortega historic dataproducts mattel decreased sixth unnecessary evenly radio christian phil kemper occupied\n"
     ]
    }
   ],
   "source": [
    "model.reset_state()\n",
    "\n",
    "start_words = 'the meaning of life is'\n",
    "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
    "print(start_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_ids[-1], skip_ids)  # 마지막 단어('is')를 시작 단어로 문장 생성\n",
    "word_ids = start_ids[:-1] + word_ids                # 'is' 앞까지의 단어를 앞부분에 추가\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)  #  실행시 마다 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science\n",
      "movements\n",
      "lumpur\n",
      "erosion\n"
     ]
    }
   ],
   "source": [
    "# 'the meaning of life' 부분 예측  :  'meaning of life is' 으로 예측 되지 않음\n",
    "for x in start_ids[:-1]:\n",
    "    x = np.array(x).reshape(1, 1)\n",
    "    score = model.predict(x).flatten()\n",
    "    p = softmax(score).flatten()\n",
    "    sampled = np.random.choice(len(p), size=1, p=p)\n",
    "    print(id_to_word[sampled[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
