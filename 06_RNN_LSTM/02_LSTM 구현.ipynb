{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM(Long Short Term Memory)\n",
    "https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 소실(Gradient Vanishing)과 기울기 폭발(Gradient Exploding)의 원인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh:\n",
      " [[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "Wh:\n",
      " [[ 1.78862847  0.43650985  0.09649747]\n",
      " [-1.8634927  -0.2773882  -0.35475898]\n",
      " [-0.08274148 -0.62700068 -0.04381817]]\n",
      "[2.4684068094579303, 3.3357049741610365, 4.783279375373182, 6.279587332087612, 8.080776465019053, 10.251163032292936, 12.936063506609896, 16.276861327786712, 20.45482961834598, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.8244073070191, 63.79612654485427, 80.07737014308985, 100.5129892205125, 126.16331847536823, 158.35920648258823, 198.7710796761195, 249.495615421267]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "N = 2  # 미니배치 크기\n",
    "H = 3  # 은닉 상태 벡터의 차원 수\n",
    "T = 20 # 시계열 데이터의 길이\n",
    "\n",
    "dh = np.ones((N,H))  # (2,3)\n",
    "print('dh:\\n',dh)\n",
    "\n",
    "np.random.seed(3)   # 재현할 수 있도록 난수의 시드 고정\n",
    "\n",
    "Wh = np.random.randn(H,H)         # 기울기 폭발 (스칼라일 경우 Wh가 1보다 큰 경우)   --> NaN, 발산\n",
    "# Wh = np.random.randn(H,H)*0.5   # 기울기 소실 (스칼라일 경우 Wh가 1보다 작은 경우) --> 0 \n",
    "print('Wh:\\n',Wh)\n",
    "\n",
    "\n",
    "# Norm은 벡터의 길이 혹은 크기를 측정하는 방법(함수)이다\n",
    "# L1 norm : 벡터의 각 요소의 절대값을 모두 합한 값\n",
    "# L2 norm : 각 요소의 제곱을 모두 합하여 제곱근을 취한 값\n",
    "# http://taewan.kim/post/norm/\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh,Wh.T)   # 미분, gradient 값\n",
    "    # print('-'*30)\n",
    "    # print(dh)\n",
    "    norm = np.sqrt(np.sum(dh**2))/N  # 평균 norm\n",
    "    # print(norm)\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list)  \n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('시간 크기(time step)')\n",
    "plt.ylabel('노름(norm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 폭발 대책 : 기울기 클리핑(gradient cliping) 함수 구현\n",
    "https://wikidocs.net/61375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1:\n",
      " [[6.49144048 2.78487283 6.76254902]\n",
      " [5.90862817 0.23981882 5.58854088]\n",
      " [2.59252447 4.15101197 2.83525082]]\n",
      "dW2:\n",
      " [[6.93137918 4.40453718 1.56867738]\n",
      " [5.44649018 7.80314765 3.06363532]\n",
      " [2.21957884 3.87971258 9.3638365 ]]\n",
      "(dW1) before: [6.49144048 2.78487283 6.76254902 5.90862817 0.23981882 5.58854088\n",
      " 2.59252447 4.15101197 2.83525082]\n",
      "(dw1) after: [1.49503731 0.64138134 1.55747605 1.36081038 0.05523244 1.28709139\n",
      " 0.59708178 0.95601551 0.65298384]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dW1 = np.random.rand(3,3)*10\n",
    "dW2 = np.random.rand(3,3)*10\n",
    "print('dW1:\\n',dW1)\n",
    "print('dW2:\\n',dW2)\n",
    "\n",
    "grads = [dW1,dW2]\n",
    "\n",
    "max_norm = 5.0   # threshold, 한계값\n",
    "\n",
    "## nn_layers.py 에 추가한다\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:  # L2 norm 구하기 , 제곱의 합의 제곱근\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    # print('rate:',rate)\n",
    "    if rate < 1:     # total_norm 이 한계값(max_norm) 보다 클경우\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "    \n",
    "\n",
    "print('(dW1) before:', dW1.flatten())\n",
    "#print('(dW2) before:', dW2.flatten())\n",
    "clip_grads(grads, max_norm)\n",
    "print('(dw1) after:', dW1.flatten())  # 값이 약간 줄어듦    \n",
    "#print('(dw2) after:', dW2.flatten())  # 값이 약간 줄어듦    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 소실 방지 : Gated RNN 인 LSTM(Long Short Term Memory)이나  GRU(Gated Recurrent Units ) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM  : RNN에 기억 셀 𝐜<sub>𝐭</sub> 과 , f , g, i, o 게이트 4개를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "# nn_layers.py 에 sigmoid 함수를 추가한다\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self,Wx,Wh,b):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx,Wh,b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self,x,h_prev,c_prev) :\n",
    "        Wx, Wh, b = self.params           # Wx,Wh : (D,4*H)로 생성하여 입력됨\n",
    "        N, H = h_prev.shape\n",
    "        \n",
    "        A = np.dot(x,Wx) + np.dot(h_prev,Wh) + b\n",
    "        \n",
    "        # 동일한 사이즈 4개로 슬라이싱 : f,g,i,o\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "        \n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "        \n",
    "        c_next = f*c_prev + g*i\n",
    "        h_next = o*np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x,h_prev,c_prev,c_next, f, g, i, o)\n",
    "        \n",
    "        return h_next,c_next\n",
    "    \n",
    "    def backward(self,dh_next,dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x,h_prev,c_prev,c_next, f, g, i, o  = self.cache\n",
    "        \n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        \n",
    "        ds = dc_next + (dh_next * o)*(1 - tanh_c_next**2)\n",
    "        \n",
    "        dc_prev = ds*f\n",
    "        \n",
    "        df = ds * c_prev\n",
    "        di = ds * g\n",
    "        dg = ds * i\n",
    "        do = dh_next * tanh_c_next\n",
    "        \n",
    "        # sigmoid 미분 : y*(1-y) \n",
    "        \n",
    "        df *=  f * (1 - f)\n",
    "        di *=  i * (1 - i)\n",
    "        do *=  o * (1 - o)\n",
    "        \n",
    "        # tanh 미분 : (1-y**2) \n",
    "        dg *= (1 - g**2)\n",
    "        \n",
    "        dA = np.hstack((df,dg,di,do))  # 수평으로 합치기 (slice의 역전파)\n",
    "        \n",
    "        # Matmul 역전파\n",
    "        dWh = np.dot(h_prev.T, dA)       \n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "        \n",
    "        # Matmul 역전파\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        \n",
    "        db = dA.sum(axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time LSTM 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeLSTM 클래스\n",
    "class TimeLSTM:\n",
    "    def __init__(self,Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx,Wh,b]\n",
    "        self.grads = [np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]\n",
    "        self.layers = None             \n",
    "        \n",
    "        self.h,self.c = None, None  \n",
    "        self.dh = None                 \n",
    "        self.statuful = stateful   \n",
    "        \n",
    "    def forward(self,xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape          # N : batch size, T : sequence length , D : input size\n",
    "        H = Wh.shape[0]             # H : hidden size,  Wh : (H,4*H)  , Wx : (D,4*H)\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N,T,H),dtype='f')  \n",
    "        \n",
    "        if not self.statuful or self.h is None:\n",
    "            self.h = np.zeros((N,H), dtype='f')\n",
    "            \n",
    "        if not self.statuful or self.c is None:\n",
    "            self.c = np.zeros((N,H), dtype='f')\n",
    "            \n",
    "        for t in range(T):               \n",
    "            layer = LSTM(*self.params)     \n",
    "            self.h , self.c = layer.forward(xs[:,t,:], self.h, self.c ) \n",
    "                                                      \n",
    "            hs[:,t,:] = self.h\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return hs   # 출력 (N,T,H)  \n",
    "    \n",
    "    def backward(self,dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape  \n",
    "        D = Wx.shape[0]   # Wx : (D,4*H)\n",
    "        \n",
    "        dxs = np.empty((N,T,D),dtype='f')\n",
    "        dh , dc = 0 , 0         \n",
    "        grads = [0,0,0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx,dh,dc = layer.backward(dhs[:,t,:] + dh, dc)  \n",
    "            dxs[:,t,:] = dx\n",
    "            \n",
    "            for i,grad in enumerate(layer.grads): \n",
    "                grads[i] += grad\n",
    "                \n",
    "        for i,grad in enumerate(grads) :\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs   \n",
    "\n",
    "    def set_state(self,h,c=None):\n",
    "        self.h , self.c = h,c\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.h , self.c = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM을 사용한 언어 모델 : Rnnlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Rnnlm:\n",
    "    def __init__(self,vocab_size=10000, wordvec_size=100,hidden_size=100 ):\n",
    "        V,D,H = vocab_size, wordvec_size,hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V,D) / 100).astype('f')       # Embediing 계층의 출력 (N,D)\n",
    "        lstm_Wx = (rn(D,4*H) / np.sqrt(D)).astype('f') # Xavier(=Glorot) Initializer, https://gomguard.tistory.com/184\n",
    "        lstm_Wh = (rn(H,4*H) / np.sqrt(H)).astype('f') # Xavier(=Glorot) Initializer, https://gomguard.tistory.com/184\n",
    "        lstm_b = np.zeros(4*H).astype('f')             # LSTM 계층의 출력 : (N,T,H)\n",
    "        affine_W = (rn(H,V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=True),\n",
    "            TimeAffine(affine_W,affine_b)            \n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss() \n",
    "        self.lstm_layer = self.layers[1]\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params,self.grads = [],[]\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self,xs):\n",
    "        for layer in self.layers:\n",
    "             xs = layer.forward(xs)\n",
    "        return xs        \n",
    "            \n",
    "    def forward(self,xs,ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score,ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers): # 3회\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "        \n",
    "    def save_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name,'wb') as f:\n",
    "            pickle.dump(self.params,f)\n",
    "            \n",
    "    def load_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name,'rb') as f:\n",
    "            self.params = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 퍼플렉서티 9998.04\n",
      "| 에폭 1 |  반복 21 / 9295 | 시간 2[s] | 퍼플렉서티 2743.66\n",
      "| 에폭 1 |  반복 41 / 9295 | 시간 5[s] | 퍼플렉서티 1424.40\n",
      "| 에폭 1 |  반복 61 / 9295 | 시간 7[s] | 퍼플렉서티 1053.62\n",
      "| 에폭 1 |  반복 81 / 9295 | 시간 9[s] | 퍼플렉서티 930.13\n",
      "| 에폭 1 |  반복 101 / 9295 | 시간 11[s] | 퍼플렉서티 688.88\n",
      "| 에폭 1 |  반복 121 / 9295 | 시간 14[s] | 퍼플렉서티 839.19\n",
      "| 에폭 1 |  반복 141 / 9295 | 시간 16[s] | 퍼플렉서티 734.00\n",
      "| 에폭 1 |  반복 161 / 9295 | 시간 19[s] | 퍼플렉서티 751.91\n",
      "| 에폭 1 |  반복 181 / 9295 | 시간 21[s] | 퍼플렉서티 587.93\n",
      "| 에폭 1 |  반복 201 / 9295 | 시간 24[s] | 퍼플렉서티 556.49\n",
      "| 에폭 1 |  반복 221 / 9295 | 시간 27[s] | 퍼플렉서티 558.26\n",
      "| 에폭 1 |  반복 241 / 9295 | 시간 29[s] | 퍼플렉서티 609.50\n",
      "| 에폭 1 |  반복 261 / 9295 | 시간 32[s] | 퍼플렉서티 588.51\n",
      "| 에폭 1 |  반복 281 / 9295 | 시간 34[s] | 퍼플렉서티 510.21\n",
      "| 에폭 1 |  반복 301 / 9295 | 시간 37[s] | 퍼플렉서티 411.44\n",
      "| 에폭 1 |  반복 321 / 9295 | 시간 39[s] | 퍼플렉서티 499.41\n",
      "| 에폭 1 |  반복 341 / 9295 | 시간 42[s] | 퍼플렉서티 405.68\n",
      "| 에폭 1 |  반복 361 / 9295 | 시간 45[s] | 퍼플렉서티 445.31\n",
      "| 에폭 1 |  반복 381 / 9295 | 시간 48[s] | 퍼플렉서티 452.22\n",
      "| 에폭 1 |  반복 401 / 9295 | 시간 50[s] | 퍼플렉서티 472.22\n",
      "| 에폭 1 |  반복 421 / 9295 | 시간 53[s] | 퍼플렉서티 412.48\n",
      "| 에폭 1 |  반복 441 / 9295 | 시간 55[s] | 퍼플렉서티 398.92\n",
      "| 에폭 1 |  반복 461 / 9295 | 시간 58[s] | 퍼플렉서티 325.40\n",
      "| 에폭 1 |  반복 481 / 9295 | 시간 60[s] | 퍼플렉서티 362.17\n",
      "| 에폭 1 |  반복 501 / 9295 | 시간 63[s] | 퍼플렉서티 352.50\n",
      "| 에폭 1 |  반복 521 / 9295 | 시간 66[s] | 퍼플렉서티 394.96\n",
      "| 에폭 1 |  반복 541 / 9295 | 시간 68[s] | 퍼플렉서티 338.57\n",
      "| 에폭 1 |  반복 561 / 9295 | 시간 71[s] | 퍼플렉서티 343.05\n",
      "| 에폭 1 |  반복 581 / 9295 | 시간 73[s] | 퍼플렉서티 281.20\n",
      "| 에폭 1 |  반복 601 / 9295 | 시간 75[s] | 퍼플렉서티 373.18\n",
      "| 에폭 1 |  반복 621 / 9295 | 시간 78[s] | 퍼플렉서티 365.44\n",
      "| 에폭 1 |  반복 641 / 9295 | 시간 80[s] | 퍼플렉서티 304.59\n",
      "| 에폭 1 |  반복 661 / 9295 | 시간 83[s] | 퍼플렉서티 263.11\n",
      "| 에폭 1 |  반복 681 / 9295 | 시간 85[s] | 퍼플렉서티 256.20\n",
      "| 에폭 1 |  반복 701 / 9295 | 시간 87[s] | 퍼플렉서티 310.30\n",
      "| 에폭 1 |  반복 721 / 9295 | 시간 90[s] | 퍼플렉서티 295.19\n",
      "| 에폭 1 |  반복 741 / 9295 | 시간 92[s] | 퍼플렉서티 298.36\n",
      "| 에폭 1 |  반복 761 / 9295 | 시간 94[s] | 퍼플렉서티 340.52\n",
      "| 에폭 1 |  반복 781 / 9295 | 시간 96[s] | 퍼플렉서티 349.35\n",
      "| 에폭 1 |  반복 801 / 9295 | 시간 99[s] | 퍼플렉서티 279.55\n",
      "| 에폭 1 |  반복 821 / 9295 | 시간 101[s] | 퍼플렉서티 293.36\n",
      "| 에폭 1 |  반복 841 / 9295 | 시간 103[s] | 퍼플렉서티 279.99\n",
      "| 에폭 1 |  반복 861 / 9295 | 시간 106[s] | 퍼플렉서티 289.24\n",
      "| 에폭 1 |  반복 881 / 9295 | 시간 108[s] | 퍼플렉서티 288.91\n",
      "| 에폭 1 |  반복 901 / 9295 | 시간 111[s] | 퍼플렉서티 293.64\n",
      "| 에폭 1 |  반복 921 / 9295 | 시간 113[s] | 퍼플렉서티 310.34\n",
      "| 에폭 1 |  반복 941 / 9295 | 시간 115[s] | 퍼플렉서티 328.31\n",
      "| 에폭 1 |  반복 961 / 9295 | 시간 117[s] | 퍼플렉서티 339.32\n",
      "| 에폭 1 |  반복 981 / 9295 | 시간 119[s] | 퍼플렉서티 347.63\n",
      "| 에폭 1 |  반복 1001 / 9295 | 시간 121[s] | 퍼플렉서티 287.39\n",
      "| 에폭 1 |  반복 1021 / 9295 | 시간 124[s] | 퍼플렉서티 320.47\n",
      "| 에폭 1 |  반복 1041 / 9295 | 시간 126[s] | 퍼플렉서티 356.45\n",
      "| 에폭 1 |  반복 1061 / 9295 | 시간 128[s] | 퍼플렉서티 351.43\n",
      "| 에폭 1 |  반복 1081 / 9295 | 시간 131[s] | 퍼플렉서티 295.75\n",
      "| 에폭 1 |  반복 1101 / 9295 | 시간 134[s] | 퍼플렉서티 293.25\n",
      "| 에폭 1 |  반복 1121 / 9295 | 시간 136[s] | 퍼플렉서티 330.85\n",
      "| 에폭 1 |  반복 1141 / 9295 | 시간 138[s] | 퍼플렉서티 317.00\n",
      "| 에폭 1 |  반복 1161 / 9295 | 시간 140[s] | 퍼플렉서티 329.73\n",
      "| 에폭 1 |  반복 1181 / 9295 | 시간 142[s] | 퍼플렉서티 311.56\n",
      "| 에폭 1 |  반복 1201 / 9295 | 시간 144[s] | 퍼플렉서티 357.43\n",
      "| 에폭 1 |  반복 1221 / 9295 | 시간 147[s] | 퍼플렉서티 315.40\n",
      "| 에폭 1 |  반복 1241 / 9295 | 시간 149[s] | 퍼플렉서티 330.57\n",
      "| 에폭 1 |  반복 1261 / 9295 | 시간 151[s] | 퍼플렉서티 314.86\n",
      "| 에폭 1 |  반복 1281 / 9295 | 시간 153[s] | 퍼플렉서티 309.42\n",
      "| 에폭 1 |  반복 1301 / 9295 | 시간 155[s] | 퍼플렉서티 289.55\n",
      "| 에폭 1 |  반복 1321 / 9295 | 시간 158[s] | 퍼플렉서티 292.79\n",
      "| 에폭 1 |  반복 1341 / 9295 | 시간 160[s] | 퍼플렉서티 273.01\n",
      "| 에폭 1 |  반복 1361 / 9295 | 시간 162[s] | 퍼플렉서티 318.85\n",
      "| 에폭 1 |  반복 1381 / 9295 | 시간 164[s] | 퍼플렉서티 239.27\n",
      "| 에폭 1 |  반복 1401 / 9295 | 시간 166[s] | 퍼플렉서티 307.25\n",
      "| 에폭 1 |  반복 1421 / 9295 | 시간 169[s] | 퍼플렉서티 296.74\n",
      "| 에폭 1 |  반복 1441 / 9295 | 시간 172[s] | 퍼플렉서티 290.85\n",
      "| 에폭 1 |  반복 1461 / 9295 | 시간 174[s] | 퍼플렉서티 331.10\n",
      "| 에폭 1 |  반복 1481 / 9295 | 시간 176[s] | 퍼플렉서티 281.67\n",
      "| 에폭 1 |  반복 1501 / 9295 | 시간 179[s] | 퍼플렉서티 293.59\n",
      "| 에폭 1 |  반복 1521 / 9295 | 시간 182[s] | 퍼플렉서티 258.26\n",
      "| 에폭 1 |  반복 1541 / 9295 | 시간 184[s] | 퍼플렉서티 234.89\n",
      "| 에폭 1 |  반복 1561 / 9295 | 시간 186[s] | 퍼플렉서티 297.26\n",
      "| 에폭 1 |  반복 1581 / 9295 | 시간 188[s] | 퍼플렉서티 281.92\n",
      "| 에폭 1 |  반복 1601 / 9295 | 시간 191[s] | 퍼플렉서티 289.86\n",
      "| 에폭 1 |  반복 1621 / 9295 | 시간 194[s] | 퍼플렉서티 245.93\n",
      "| 에폭 1 |  반복 1641 / 9295 | 시간 196[s] | 퍼플렉서티 275.00\n",
      "| 에폭 1 |  반복 1661 / 9295 | 시간 198[s] | 퍼플렉서티 251.10\n",
      "| 에폭 1 |  반복 1681 / 9295 | 시간 201[s] | 퍼플렉서티 239.03\n",
      "| 에폭 1 |  반복 1701 / 9295 | 시간 203[s] | 퍼플렉서티 233.19\n",
      "| 에폭 1 |  반복 1721 / 9295 | 시간 206[s] | 퍼플렉서티 283.46\n",
      "| 에폭 1 |  반복 1741 / 9295 | 시간 208[s] | 퍼플렉서티 332.76\n",
      "| 에폭 1 |  반복 1761 / 9295 | 시간 211[s] | 퍼플렉서티 259.55\n",
      "| 에폭 1 |  반복 1781 / 9295 | 시간 213[s] | 퍼플렉서티 263.37\n",
      "| 에폭 1 |  반복 1801 / 9295 | 시간 215[s] | 퍼플렉서티 273.62\n",
      "| 에폭 1 |  반복 1821 / 9295 | 시간 217[s] | 퍼플렉서티 279.49\n",
      "| 에폭 1 |  반복 1841 / 9295 | 시간 219[s] | 퍼플렉서티 276.41\n",
      "| 에폭 1 |  반복 1861 / 9295 | 시간 221[s] | 퍼플렉서티 302.76\n",
      "| 에폭 1 |  반복 1881 / 9295 | 시간 224[s] | 퍼플렉서티 260.29\n",
      "| 에폭 1 |  반복 1901 / 9295 | 시간 226[s] | 퍼플렉서티 313.85\n",
      "| 에폭 1 |  반복 1921 / 9295 | 시간 228[s] | 퍼플렉서티 299.17\n",
      "| 에폭 1 |  반복 1941 / 9295 | 시간 231[s] | 퍼플렉서티 264.98\n",
      "| 에폭 1 |  반복 1961 / 9295 | 시간 233[s] | 퍼플렉서티 245.25\n",
      "| 에폭 1 |  반복 1981 / 9295 | 시간 235[s] | 퍼플렉서티 257.93\n",
      "| 에폭 1 |  반복 2001 / 9295 | 시간 237[s] | 퍼플렉서티 277.23\n",
      "| 에폭 1 |  반복 2021 / 9295 | 시간 240[s] | 퍼플렉서티 233.53\n",
      "| 에폭 1 |  반복 2041 / 9295 | 시간 242[s] | 퍼플렉서티 269.35\n",
      "| 에폭 1 |  반복 2061 / 9295 | 시간 244[s] | 퍼플렉서티 238.11\n",
      "| 에폭 1 |  반복 2081 / 9295 | 시간 247[s] | 퍼플렉서티 231.16\n",
      "| 에폭 1 |  반복 2101 / 9295 | 시간 249[s] | 퍼플렉서티 229.89\n",
      "| 에폭 1 |  반복 2121 / 9295 | 시간 251[s] | 퍼플렉서티 227.37\n",
      "| 에폭 1 |  반복 2141 / 9295 | 시간 253[s] | 퍼플렉서티 226.61\n",
      "| 에폭 1 |  반복 2161 / 9295 | 시간 256[s] | 퍼플렉서티 239.77\n",
      "| 에폭 1 |  반복 2181 / 9295 | 시간 258[s] | 퍼플렉서티 194.87\n",
      "| 에폭 1 |  반복 2201 / 9295 | 시간 260[s] | 퍼플렉서티 231.91\n",
      "| 에폭 1 |  반복 2221 / 9295 | 시간 262[s] | 퍼플렉서티 227.03\n",
      "| 에폭 1 |  반복 2241 / 9295 | 시간 264[s] | 퍼플렉서티 168.76\n",
      "| 에폭 1 |  반복 2261 / 9295 | 시간 266[s] | 퍼플렉서티 198.42\n",
      "| 에폭 1 |  반복 2281 / 9295 | 시간 268[s] | 퍼플렉서티 198.56\n",
      "| 에폭 1 |  반복 2301 / 9295 | 시간 270[s] | 퍼플렉서티 270.01\n",
      "| 에폭 1 |  반복 2321 / 9295 | 시간 272[s] | 퍼플렉서티 247.03\n",
      "| 에폭 1 |  반복 2341 / 9295 | 시간 274[s] | 퍼플렉서티 274.22\n",
      "| 에폭 1 |  반복 2361 / 9295 | 시간 276[s] | 퍼플렉서티 300.83\n",
      "| 에폭 1 |  반복 2381 / 9295 | 시간 279[s] | 퍼플렉서티 293.97\n",
      "| 에폭 1 |  반복 2401 / 9295 | 시간 281[s] | 퍼플렉서티 321.63\n",
      "| 에폭 1 |  반복 2421 / 9295 | 시간 283[s] | 퍼플렉서티 309.19\n",
      "| 에폭 1 |  반복 2441 / 9295 | 시간 285[s] | 퍼플렉서티 337.93\n",
      "| 에폭 1 |  반복 2461 / 9295 | 시간 287[s] | 퍼플렉서티 319.05\n",
      "| 에폭 1 |  반복 2481 / 9295 | 시간 290[s] | 퍼플렉서티 275.22\n",
      "| 에폭 1 |  반복 2501 / 9295 | 시간 292[s] | 퍼플렉서티 234.13\n",
      "| 에폭 1 |  반복 2521 / 9295 | 시간 294[s] | 퍼플렉서티 199.40\n",
      "| 에폭 1 |  반복 2541 / 9295 | 시간 296[s] | 퍼플렉서티 171.44\n",
      "| 에폭 1 |  반복 2561 / 9295 | 시간 299[s] | 퍼플렉서티 244.29\n",
      "| 에폭 1 |  반복 2581 / 9295 | 시간 301[s] | 퍼플렉서티 202.74\n",
      "| 에폭 1 |  반복 2601 / 9295 | 시간 303[s] | 퍼플렉서티 229.06\n",
      "| 에폭 1 |  반복 2621 / 9295 | 시간 306[s] | 퍼플렉서티 262.68\n",
      "| 에폭 1 |  반복 2641 / 9295 | 시간 308[s] | 퍼플렉서티 249.36\n",
      "| 에폭 1 |  반복 2661 / 9295 | 시간 310[s] | 퍼플렉서티 199.89\n",
      "| 에폭 1 |  반복 2681 / 9295 | 시간 313[s] | 퍼플렉서티 200.68\n",
      "| 에폭 1 |  반복 2701 / 9295 | 시간 315[s] | 퍼플렉서티 241.25\n",
      "| 에폭 1 |  반복 2721 / 9295 | 시간 317[s] | 퍼플렉서티 236.02\n",
      "| 에폭 1 |  반복 2741 / 9295 | 시간 319[s] | 퍼플렉서티 290.41\n",
      "| 에폭 1 |  반복 2761 / 9295 | 시간 322[s] | 퍼플렉서티 264.13\n",
      "| 에폭 1 |  반복 2781 / 9295 | 시간 324[s] | 퍼플렉서티 253.04\n",
      "| 에폭 1 |  반복 2801 / 9295 | 시간 327[s] | 퍼플렉서티 205.40\n",
      "| 에폭 1 |  반복 2821 / 9295 | 시간 329[s] | 퍼플렉서티 245.78\n",
      "| 에폭 1 |  반복 2841 / 9295 | 시간 331[s] | 퍼플렉서티 196.30\n",
      "| 에폭 1 |  반복 2861 / 9295 | 시간 334[s] | 퍼플렉서티 211.77\n",
      "| 에폭 1 |  반복 2881 / 9295 | 시간 336[s] | 퍼플렉서티 241.62\n",
      "| 에폭 1 |  반복 2901 / 9295 | 시간 338[s] | 퍼플렉서티 273.25\n",
      "| 에폭 1 |  반복 2921 / 9295 | 시간 340[s] | 퍼플렉서티 240.40\n",
      "| 에폭 1 |  반복 2941 / 9295 | 시간 343[s] | 퍼플렉서티 221.54\n",
      "| 에폭 1 |  반복 2961 / 9295 | 시간 345[s] | 퍼플렉서티 256.82\n",
      "| 에폭 1 |  반복 2981 / 9295 | 시간 347[s] | 퍼플렉서티 250.55\n",
      "| 에폭 1 |  반복 3001 / 9295 | 시간 350[s] | 퍼플렉서티 221.00\n",
      "| 에폭 1 |  반복 3021 / 9295 | 시간 352[s] | 퍼플렉서티 198.08\n",
      "| 에폭 1 |  반복 3041 / 9295 | 시간 354[s] | 퍼플렉서티 250.30\n",
      "| 에폭 1 |  반복 3061 / 9295 | 시간 356[s] | 퍼플렉서티 208.12\n",
      "| 에폭 1 |  반복 3081 / 9295 | 시간 359[s] | 퍼플렉서티 237.86\n",
      "| 에폭 1 |  반복 3101 / 9295 | 시간 361[s] | 퍼플렉서티 206.70\n",
      "| 에폭 1 |  반복 3121 / 9295 | 시간 363[s] | 퍼플렉서티 235.21\n",
      "| 에폭 1 |  반복 3141 / 9295 | 시간 365[s] | 퍼플렉서티 230.99\n",
      "| 에폭 1 |  반복 3161 / 9295 | 시간 367[s] | 퍼플렉서티 229.05\n",
      "| 에폭 1 |  반복 3181 / 9295 | 시간 370[s] | 퍼플렉서티 186.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 3201 / 9295 | 시간 372[s] | 퍼플렉서티 254.69\n",
      "| 에폭 1 |  반복 3221 / 9295 | 시간 374[s] | 퍼플렉서티 245.85\n",
      "| 에폭 1 |  반복 3241 / 9295 | 시간 376[s] | 퍼플렉서티 243.30\n",
      "| 에폭 1 |  반복 3261 / 9295 | 시간 379[s] | 퍼플렉서티 241.75\n",
      "| 에폭 1 |  반복 3281 / 9295 | 시간 381[s] | 퍼플렉서티 233.75\n",
      "| 에폭 1 |  반복 3301 / 9295 | 시간 383[s] | 퍼플렉서티 220.27\n",
      "| 에폭 1 |  반복 3321 / 9295 | 시간 385[s] | 퍼플렉서티 214.15\n",
      "| 에폭 1 |  반복 3341 / 9295 | 시간 388[s] | 퍼플렉서티 185.99\n",
      "| 에폭 1 |  반복 3361 / 9295 | 시간 390[s] | 퍼플렉서티 181.62\n",
      "| 에폭 1 |  반복 3381 / 9295 | 시간 393[s] | 퍼플렉서티 197.62\n",
      "| 에폭 1 |  반복 3401 / 9295 | 시간 396[s] | 퍼플렉서티 218.00\n",
      "| 에폭 1 |  반복 3421 / 9295 | 시간 399[s] | 퍼플렉서티 216.59\n",
      "| 에폭 1 |  반복 3441 / 9295 | 시간 401[s] | 퍼플렉서티 273.12\n",
      "| 에폭 1 |  반복 3461 / 9295 | 시간 404[s] | 퍼플렉서티 261.68\n",
      "| 에폭 1 |  반복 3481 / 9295 | 시간 407[s] | 퍼플렉서티 261.79\n",
      "| 에폭 1 |  반복 3501 / 9295 | 시간 410[s] | 퍼플렉서티 221.82\n",
      "| 에폭 1 |  반복 3521 / 9295 | 시간 412[s] | 퍼플렉서티 285.12\n",
      "| 에폭 1 |  반복 3541 / 9295 | 시간 415[s] | 퍼플렉서티 277.96\n",
      "| 에폭 1 |  반복 3561 / 9295 | 시간 418[s] | 퍼플렉서티 274.82\n",
      "| 에폭 1 |  반복 3581 / 9295 | 시간 420[s] | 퍼플렉서티 209.30\n",
      "| 에폭 1 |  반복 3601 / 9295 | 시간 423[s] | 퍼플렉서티 201.58\n",
      "| 에폭 1 |  반복 3621 / 9295 | 시간 425[s] | 퍼플렉서티 203.13\n",
      "| 에폭 1 |  반복 3641 / 9295 | 시간 427[s] | 퍼플렉서티 188.96\n",
      "| 에폭 1 |  반복 3661 / 9295 | 시간 430[s] | 퍼플렉서티 195.37\n",
      "| 에폭 1 |  반복 3681 / 9295 | 시간 433[s] | 퍼플렉서티 237.67\n",
      "| 에폭 1 |  반복 3701 / 9295 | 시간 435[s] | 퍼플렉서티 209.84\n",
      "| 에폭 1 |  반복 3721 / 9295 | 시간 438[s] | 퍼플렉서티 192.81\n",
      "| 에폭 1 |  반복 3741 / 9295 | 시간 440[s] | 퍼플렉서티 261.33\n",
      "| 에폭 1 |  반복 3761 / 9295 | 시간 443[s] | 퍼플렉서티 285.11\n",
      "| 에폭 1 |  반복 3781 / 9295 | 시간 445[s] | 퍼플렉서티 275.12\n",
      "| 에폭 1 |  반복 3801 / 9295 | 시간 447[s] | 퍼플렉서티 279.96\n",
      "| 에폭 1 |  반복 3821 / 9295 | 시간 449[s] | 퍼플렉서티 214.10\n",
      "| 에폭 1 |  반복 3841 / 9295 | 시간 452[s] | 퍼플렉서티 201.64\n",
      "| 에폭 1 |  반복 3861 / 9295 | 시간 454[s] | 퍼플렉서티 199.25\n",
      "| 에폭 1 |  반복 3881 / 9295 | 시간 456[s] | 퍼플렉서티 204.10\n",
      "| 에폭 1 |  반복 3901 / 9295 | 시간 458[s] | 퍼플렉서티 206.88\n",
      "| 에폭 1 |  반복 3921 / 9295 | 시간 461[s] | 퍼플렉서티 201.94\n",
      "| 에폭 1 |  반복 3941 / 9295 | 시간 463[s] | 퍼플렉서티 202.42\n",
      "| 에폭 1 |  반복 3961 / 9295 | 시간 465[s] | 퍼플렉서티 161.37\n",
      "| 에폭 1 |  반복 3981 / 9295 | 시간 467[s] | 퍼플렉서티 189.16\n",
      "| 에폭 1 |  반복 4001 / 9295 | 시간 469[s] | 퍼플렉서티 210.52\n",
      "| 에폭 1 |  반복 4021 / 9295 | 시간 472[s] | 퍼플렉서티 181.73\n",
      "| 에폭 1 |  반복 4041 / 9295 | 시간 474[s] | 퍼플렉서티 200.35\n",
      "| 에폭 1 |  반복 4061 / 9295 | 시간 476[s] | 퍼플렉서티 236.56\n",
      "| 에폭 1 |  반복 4081 / 9295 | 시간 478[s] | 퍼플렉서티 268.32\n",
      "| 에폭 1 |  반복 4101 / 9295 | 시간 480[s] | 퍼플렉서티 229.55\n",
      "| 에폭 1 |  반복 4121 / 9295 | 시간 483[s] | 퍼플렉서티 260.39\n",
      "| 에폭 1 |  반복 4141 / 9295 | 시간 485[s] | 퍼플렉서티 252.21\n",
      "| 에폭 1 |  반복 4161 / 9295 | 시간 488[s] | 퍼플렉서티 251.72\n",
      "| 에폭 1 |  반복 4181 / 9295 | 시간 490[s] | 퍼플렉서티 256.08\n",
      "| 에폭 1 |  반복 4201 / 9295 | 시간 493[s] | 퍼플렉서티 246.24\n",
      "| 에폭 1 |  반복 4221 / 9295 | 시간 495[s] | 퍼플렉서티 236.86\n",
      "| 에폭 1 |  반복 4241 / 9295 | 시간 497[s] | 퍼플렉서티 256.65\n",
      "| 에폭 1 |  반복 4261 / 9295 | 시간 500[s] | 퍼플렉서티 235.88\n",
      "| 에폭 1 |  반복 4281 / 9295 | 시간 503[s] | 퍼플렉서티 215.75\n",
      "| 에폭 1 |  반복 4301 / 9295 | 시간 505[s] | 퍼플렉서티 256.34\n",
      "| 에폭 1 |  반복 4321 / 9295 | 시간 507[s] | 퍼플렉서티 251.31\n",
      "| 에폭 1 |  반복 4341 / 9295 | 시간 510[s] | 퍼플렉서티 236.28\n",
      "| 에폭 1 |  반복 4361 / 9295 | 시간 512[s] | 퍼플렉서티 184.83\n",
      "| 에폭 1 |  반복 4381 / 9295 | 시간 514[s] | 퍼플렉서티 189.01\n",
      "| 에폭 1 |  반복 4401 / 9295 | 시간 516[s] | 퍼플렉서티 252.26\n",
      "| 에폭 1 |  반복 4421 / 9295 | 시간 518[s] | 퍼플렉서티 270.77\n",
      "| 에폭 1 |  반복 4441 / 9295 | 시간 520[s] | 퍼플렉서티 226.64\n",
      "| 에폭 1 |  반복 4461 / 9295 | 시간 523[s] | 퍼플렉서티 246.63\n",
      "| 에폭 1 |  반복 4481 / 9295 | 시간 525[s] | 퍼플렉서티 198.14\n",
      "| 에폭 1 |  반복 4501 / 9295 | 시간 527[s] | 퍼플렉서티 194.73\n",
      "| 에폭 1 |  반복 4521 / 9295 | 시간 530[s] | 퍼플렉서티 235.66\n",
      "| 에폭 1 |  반복 4541 / 9295 | 시간 532[s] | 퍼플렉서티 186.17\n",
      "| 에폭 1 |  반복 4561 / 9295 | 시간 534[s] | 퍼플렉서티 207.53\n",
      "| 에폭 1 |  반복 4581 / 9295 | 시간 536[s] | 퍼플렉서티 233.68\n",
      "| 에폭 1 |  반복 4601 / 9295 | 시간 538[s] | 퍼플렉서티 237.54\n",
      "| 에폭 1 |  반복 4621 / 9295 | 시간 540[s] | 퍼플렉서티 202.72\n",
      "| 에폭 1 |  반복 4641 / 9295 | 시간 543[s] | 퍼플렉서티 172.37\n",
      "| 에폭 1 |  반복 4661 / 9295 | 시간 545[s] | 퍼플렉서티 178.47\n",
      "| 에폭 1 |  반복 4681 / 9295 | 시간 548[s] | 퍼플렉서티 149.02\n",
      "| 에폭 1 |  반복 4701 / 9295 | 시간 550[s] | 퍼플렉서티 160.46\n",
      "| 에폭 1 |  반복 4721 / 9295 | 시간 553[s] | 퍼플렉서티 154.51\n",
      "| 에폭 1 |  반복 4741 / 9295 | 시간 555[s] | 퍼플렉서티 181.94\n",
      "| 에폭 1 |  반복 4761 / 9295 | 시간 558[s] | 퍼플렉서티 214.65\n",
      "| 에폭 1 |  반복 4781 / 9295 | 시간 560[s] | 퍼플렉서티 225.98\n",
      "| 에폭 1 |  반복 4801 / 9295 | 시간 562[s] | 퍼플렉서티 217.30\n",
      "| 에폭 1 |  반복 4821 / 9295 | 시간 565[s] | 퍼플렉서티 201.07\n",
      "| 에폭 1 |  반복 4841 / 9295 | 시간 567[s] | 퍼플렉서티 221.28\n",
      "| 에폭 1 |  반복 4861 / 9295 | 시간 569[s] | 퍼플렉서티 156.58\n",
      "| 에폭 1 |  반복 4881 / 9295 | 시간 572[s] | 퍼플렉서티 190.63\n",
      "| 에폭 1 |  반복 4901 / 9295 | 시간 574[s] | 퍼플렉서티 215.85\n",
      "| 에폭 1 |  반복 4921 / 9295 | 시간 576[s] | 퍼플렉서티 200.48\n",
      "| 에폭 1 |  반복 4941 / 9295 | 시간 578[s] | 퍼플렉서티 241.71\n",
      "| 에폭 1 |  반복 4961 / 9295 | 시간 580[s] | 퍼플렉서티 225.70\n",
      "| 에폭 1 |  반복 4981 / 9295 | 시간 582[s] | 퍼플렉서티 198.33\n",
      "| 에폭 1 |  반복 5001 / 9295 | 시간 584[s] | 퍼플렉서티 209.49\n",
      "| 에폭 1 |  반복 5021 / 9295 | 시간 587[s] | 퍼플렉서티 193.23\n",
      "| 에폭 1 |  반복 5041 / 9295 | 시간 589[s] | 퍼플렉서티 217.65\n",
      "| 에폭 1 |  반복 5061 / 9295 | 시간 591[s] | 퍼플렉서티 166.59\n",
      "| 에폭 1 |  반복 5081 / 9295 | 시간 593[s] | 퍼플렉서티 170.84\n",
      "| 에폭 1 |  반복 5101 / 9295 | 시간 595[s] | 퍼플렉서티 158.38\n",
      "| 에폭 1 |  반복 5121 / 9295 | 시간 597[s] | 퍼플렉서티 182.85\n",
      "| 에폭 1 |  반복 5141 / 9295 | 시간 599[s] | 퍼플렉서티 203.87\n",
      "| 에폭 1 |  반복 5161 / 9295 | 시간 602[s] | 퍼플렉서티 198.32\n",
      "| 에폭 1 |  반복 5181 / 9295 | 시간 604[s] | 퍼플렉서티 181.82\n",
      "| 에폭 1 |  반복 5201 / 9295 | 시간 606[s] | 퍼플렉서티 206.37\n",
      "| 에폭 1 |  반복 5221 / 9295 | 시간 608[s] | 퍼플렉서티 189.74\n",
      "| 에폭 1 |  반복 5241 / 9295 | 시간 610[s] | 퍼플렉서티 191.59\n",
      "| 에폭 1 |  반복 5261 / 9295 | 시간 613[s] | 퍼플렉서티 192.63\n",
      "| 에폭 1 |  반복 5281 / 9295 | 시간 615[s] | 퍼플렉서티 184.17\n",
      "| 에폭 1 |  반복 5301 / 9295 | 시간 618[s] | 퍼플렉서티 206.57\n",
      "| 에폭 1 |  반복 5321 / 9295 | 시간 620[s] | 퍼플렉서티 113.66\n",
      "| 에폭 1 |  반복 5341 / 9295 | 시간 622[s] | 퍼플렉서티 163.66\n",
      "| 에폭 1 |  반복 5361 / 9295 | 시간 624[s] | 퍼플렉서티 198.69\n",
      "| 에폭 1 |  반복 5381 / 9295 | 시간 626[s] | 퍼플렉서티 203.57\n",
      "| 에폭 1 |  반복 5401 / 9295 | 시간 628[s] | 퍼플렉서티 173.09\n",
      "| 에폭 1 |  반복 5421 / 9295 | 시간 631[s] | 퍼플렉서티 185.17\n",
      "| 에폭 1 |  반복 5441 / 9295 | 시간 634[s] | 퍼플렉서티 210.12\n",
      "| 에폭 1 |  반복 5461 / 9295 | 시간 637[s] | 퍼플렉서티 165.19\n",
      "| 에폭 1 |  반복 5481 / 9295 | 시간 639[s] | 퍼플렉서티 177.12\n",
      "| 에폭 1 |  반복 5501 / 9295 | 시간 642[s] | 퍼플렉서티 199.48\n",
      "| 에폭 1 |  반복 5521 / 9295 | 시간 645[s] | 퍼플렉서티 233.08\n",
      "| 에폭 1 |  반복 5541 / 9295 | 시간 647[s] | 퍼플렉서티 214.88\n",
      "| 에폭 1 |  반복 5561 / 9295 | 시간 650[s] | 퍼플렉서티 189.43\n",
      "| 에폭 1 |  반복 5581 / 9295 | 시간 653[s] | 퍼플렉서티 215.05\n",
      "| 에폭 1 |  반복 5601 / 9295 | 시간 656[s] | 퍼플렉서티 194.26\n",
      "| 에폭 1 |  반복 5621 / 9295 | 시간 659[s] | 퍼플렉서티 224.71\n",
      "| 에폭 1 |  반복 5641 / 9295 | 시간 661[s] | 퍼플렉서티 226.52\n",
      "| 에폭 1 |  반복 5661 / 9295 | 시간 664[s] | 퍼플렉서티 222.24\n",
      "| 에폭 1 |  반복 5681 / 9295 | 시간 667[s] | 퍼플렉서티 174.57\n",
      "| 에폭 1 |  반복 5701 / 9295 | 시간 670[s] | 퍼플렉서티 149.59\n",
      "| 에폭 1 |  반복 5721 / 9295 | 시간 673[s] | 퍼플렉서티 170.47\n",
      "| 에폭 1 |  반복 5741 / 9295 | 시간 677[s] | 퍼플렉서티 185.51\n",
      "| 에폭 1 |  반복 5761 / 9295 | 시간 680[s] | 퍼플렉서티 189.39\n",
      "| 에폭 1 |  반복 5781 / 9295 | 시간 683[s] | 퍼플렉서티 192.21\n",
      "| 에폭 1 |  반복 5801 / 9295 | 시간 685[s] | 퍼플렉서티 150.20\n",
      "| 에폭 1 |  반복 5821 / 9295 | 시간 687[s] | 퍼플렉서티 175.09\n",
      "| 에폭 1 |  반복 5841 / 9295 | 시간 690[s] | 퍼플렉서티 214.60\n"
     ]
    }
   ],
   "source": [
    "from nn_layers import sigmoid,SGD, TimeEmbedding, TimeAffine, TimeSoftmaxWithLoss, RnnlmTrainer\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 5  # RNN을 펼치는 크기\n",
    "lr = 20.0\n",
    "# max_epoch = 4  #  약 30분 소요 , PPL : 약 136.07\n",
    "max_epoch = 1\n",
    "max_grad = 0.25  #  기울기 클리핑을 위한 threshold 값\n",
    "\n",
    "# 학습 데이터 읽기 : 전체 데이터 사용\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test,_,_ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]   # 출력（정답 레이블）\n",
    "\n",
    "# 모델 생성\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "\n",
    "# 기울기 클리핑을 적용하여 학습\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval=20)\n",
    "trainer.plot(ylim=(0,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('퍼플렉서티 평가 중 ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)  # test 데이터로 평가\n",
    "print('테스트 퍼플렉서티: ', ppl_test)\n",
    "\n",
    "# 매개변수 저장\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
