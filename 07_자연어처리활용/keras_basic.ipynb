{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Basic\n",
    "### * Sequential Layer 의 주요 레이어 정리 \n",
    "#### [1]  Dense Layer : Fully Connected Layer, layer의 입력과 출력 사이에 있는 모든 뉴런이 서로 연결 되는 Layer\n",
    "#### [2]  Flatten Layer : 다차원(4차원)을 2차원으로 축소하여 FC Layer에 전달한다\n",
    "#### [3]  Conv2D Layer : 이미지 특징을 추출하는 Convolution Layer \n",
    "#### [4]  MaxPool2D Layer : 중요 데이터(값이 큰 데이터)를 subsampling 하는 Layer\n",
    "#### [5]  Dropout Layer : 학습시 신경망의 과적합을 막기위해 일부 뉴런을 제거하는 Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 7)                 14        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# keras_basic\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# from tensorflow.keras import Sequential,Dense,...\n",
    "# from keras import seq\n",
    "# Sequential()\n",
    "\n",
    "x_train = [1.0, 2.0, 3.0, 4.0, 5.0]   # (5,) \n",
    "y_train = [1.1, 2.2, 3.3, 4.4, 5.5]   # (5,) \n",
    "\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Dense())\n",
    "# model.add(t.keras.layers.Dense(units=1))\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # (?,1) * (1,7) = (?,7)   , Param :14 (W:7, b:7)\n",
    "    # (5,1) * (1,7) = (5,7)   , Param :14 (W:7, b:7)\n",
    "    tf.keras.layers.Dense(units=7, activation='relu', input_shape=(1,)), # input data의 shape을 (?,1) \n",
    "    \n",
    "    # (?,7) * (7,1) = (?,1)   , Pram : 8  (W:7, b:1)\n",
    "    # (5,7) * (7,1) = (5,1)   , Pram : 8  (W:7, b:1)\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 157ms/sample - loss: 16.6922\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 16.2386\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 15.8122\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 15.4126\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 15.0393\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 14.6915\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 14.3687\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 14.0694\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 1ms/sample - loss: 13.7941\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 4ms/sample - loss: 13.5448\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 13.3195\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 13.1082\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 12.9098\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 12.7235\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 12.5481\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 12.3825\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 12.2255\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 12.0759\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 11.9325\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 11.7939\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 11.6590\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 11.5263\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 11.3948\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 11.2634\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 11.1312\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 10.9970\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 10.8601\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 10.7196\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 10.5743\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 10.4237\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 10.2673\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 10.1046\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 9.9352\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 9.7587\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 9.5747\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 9.3829\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 9.1828\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 8.9743\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 8.7574\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 8.5319\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 8.2977\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 8.0547\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 7.8030\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 7.5427\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 7.2738\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 6.9968\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 6.7120\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 6.4199\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 4ms/sample - loss: 6.1212\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 5.8166\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 5.5071\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 5.1937\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 4.8775\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 4.5599\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 4.2423\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 3.9262\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 3.6132\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 1ms/sample - loss: 3.3050\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 3.0033\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 2.7099\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 2.4265\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 2.1549\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 1.8968\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 1.6537\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 1.4271\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 1.2183\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 1.0283\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.8581\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.7080\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.5783\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.4689\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.3792\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.3085\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2554\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2184\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.1956\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.1850\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.1843\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 0.1911\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 1ms/sample - loss: 0.2033\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2186\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2352\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2513\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2656\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 1ms/sample - loss: 0.2772\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2853\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2898\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2905\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2878\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2820\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2737\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2636\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2521\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2401\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2279\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2161\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.2051\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.1950\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 0.1862\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 2ms/sample - loss: 0.1786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c0e4bb4b48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8189552],\n",
       "       [2.675808 ],\n",
       "       [3.5326607],\n",
       "       [4.3895135],\n",
       "       [5.2463665]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "5/1 [======================================================================================================================================================] - 0s 18ms/sample - loss: 0.1724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17237213253974915"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 7,055\n",
      "Trainable params: 7,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# X,Y input data \n",
    "x_train = np.array([[1.0,   2.0,  3.0,  4.0,  5.0],\n",
    "                    [6.0,   7.0,  8.0,  9.0, 10.0],\n",
    "                    [11.0, 12.0, 13.0, 14.0, 15.0]])  # (3,5)\n",
    "                    \n",
    "                    \n",
    "y_train = np.array([[1.1,   2.2,  3.3,  4.4,  5.5],\n",
    "                    [6.1,   7.2,  8.3,  9.4, 10.5],\n",
    "                    [11.1, 12.2, 13.3, 14.4, 15.5]]) # (3,5)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # (3,1) * (1,100) = (3,100)   , Param :200 (W:1*100, b:100)\n",
    "    # tf.keras.layers.Dense(units=100, activation='relu', input_shape=(1,)), # input data의 shape을 (?,1) \n",
    "\n",
    "    # (3,5) * (5,100) = (3,100)   , Param :600 (W:5*100, b:100)\n",
    "    tf.keras.layers.Dense(units=100, activation='relu', input_shape=(5,)), # input data의 shape을 (?,5) \n",
    "\n",
    "    \n",
    "    # (3,100) * (100,50) = (3,50)   , Param : 5050 (W:100*50, b:50)\n",
    "    tf.keras.layers.Dense(units=50, activation='relu' ), \n",
    "    \n",
    "    # (3,50) * (50,25) = (3,25)   , Param : 5050 (W:50*25, b:25)\n",
    "    tf.keras.layers.Dense(units=25, activation='relu' ),\n",
    "    \n",
    "    # (3,25) * (25,5) = (3,5)   , Param : 130 (W:25*5, b:5)\n",
    "    tf.keras.layers.Dense(units=5 , activation='relu' ),\n",
    "    \n",
    "#     # (3,5) * (5,1) = (3,1)   , Pram : 6  (W:5*1, b:1)\n",
    "#     tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 179ms/sample - loss: 76.4652\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 57.5874\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 38.4393\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 26.0011\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 32.3374\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 36.6849\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 31.2214\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 26.5416\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 25.7046\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 26.9832\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 28.4095\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 3ms/sample - loss: 29.0536\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 2ms/sample - loss: 28.7909\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 27.9278\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 26.8890\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 25.9994\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 25.4691\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 25.3594\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 25.5443\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 25.8262\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 2ms/sample - loss: 25.9900\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 25.9553\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 25.7053\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 25.3587\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 666us/sample - loss: 25.0523\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 666us/sample - loss: 24.8704\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.8439\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.9508\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 25.1145\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 25.2278\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 25.2000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 995us/sample - loss: 25.0251\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.7944\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.6309\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.6046\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.6944\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.8147\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.8783\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 24.8491\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.7529\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.6488\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.5872\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5827\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 24.6153\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.6520\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.6693\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 664us/sample - loss: 24.6614\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.6349\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.6015\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5721\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 662us/sample - loss: 24.5556\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 2ms/sample - loss: 24.5558\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5693\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 662us/sample - loss: 24.5849\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 993us/sample - loss: 24.5899\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 664us/sample - loss: 24.5787\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 666us/sample - loss: 24.5575\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 995us/sample - loss: 24.5389\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.5318\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 666us/sample - loss: 24.5362\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 2ms/sample - loss: 24.5447\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.5495\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 2ms/sample - loss: 24.5473\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 999us/sample - loss: 24.5398\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.5313\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 826us/sample - loss: 24.5252\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5227\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 2ms/sample - loss: 24.5231\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5247\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 664us/sample - loss: 24.5259\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 664us/sample - loss: 24.5252\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 2ms/sample - loss: 24.5221\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 996us/sample - loss: 24.5176\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 996us/sample - loss: 24.5138\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.5123\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 413us/sample - loss: 24.5131\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5144\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5144\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5125\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5097\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5075\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.5064\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.5063\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.5064\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.5059\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5050\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5036\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5022\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.5011\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 666us/sample - loss: 24.5005\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.5002\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.4999\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 999us/sample - loss: 24.4993\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 24.4982\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 999us/sample - loss: 24.4971\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 1ms/sample - loss: 24.4962\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 664us/sample - loss: 24.4957\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 665us/sample - loss: 24.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 998us/sample - loss: 24.4948\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 997us/sample - loss: 24.4941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c0e6318748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  3.6420217,  4.1769757,  5.3974295],\n",
       "       [ 0.       ,  0.       ,  8.357432 ,  9.351834 , 10.449328 ],\n",
       "       [ 0.       ,  0.       , 13.102687 , 14.531496 , 15.586285 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "3/1 [==========================================================================================] - 0s 60ms/sample - loss: 24.4933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24.49327278137207"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### [2] Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 10)\n",
      "(1000, 7840)\n"
     ]
    }
   ],
   "source": [
    "# 4차원 데이터를 2차원으로 축소하여 FC Layer에 전달\n",
    "data = np.arange(1000*28*28*10).reshape(1000,28,28,10).astype(np.float32)\n",
    "print(data.shape) # (None, 28, 28, 10)\n",
    "layer = tf.keras.layers.Flatten()\n",
    "outputs = layer(data)\n",
    "print(outputs.shape) # (None, 28*28*10) = (None,7840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 3차원 데이터를 2차원으로 축소하여 FC Layer에 전달\n",
    "data = np.arange(1000*28*28).reshape(1000,28,28).astype(np.float32)\n",
    "print(data.shape) # (None, 28, 28)\n",
    "layer = tf.keras.layers.Flatten()\n",
    "outputs = layer(data)\n",
    "print(outputs.shape) # (None, 28*28) = (None,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 2차원 데이터는 그대로 FC Layer에 전달\n",
    "data = np.arange(28*28).reshape(28,28).astype(np.float32)\n",
    "print(data.shape) # (28, 28)\n",
    "layer = tf.keras.layers.Flatten()\n",
    "outputs = layer(data)\n",
    "print(outputs.shape) # (28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "# The Dropout layer randomly sets input units to 0 with a frequency of rate \n",
    "# at each step during training time, which helps prevent overfitting. \n",
    "# Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "\n",
    "# Note that the Dropout layer only applies when training is set to True such that\n",
    "# no values are dropped during inference. When using model.fit, training will be\n",
    "# appropriately set to True automatically, and in other contexts, you can set \n",
    "# the kwarg explicitly to True when calling the layer.\n",
    "\n",
    "# (This is in contrast to setting trainable=False for a Dropout layer.\n",
    "# trainable does not affect the layer's behavior, as Dropout does not have any \n",
    "# variables/weights that can be frozen during training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]\n",
      " [6. 7.]\n",
      " [8. 9.]]\n",
      "tf.Tensor(\n",
      "[[ 0.         0.       ]\n",
      " [ 2.857143   4.285714 ]\n",
      " [ 5.714286   7.1428576]\n",
      " [ 8.571428  10.       ]\n",
      " [11.428572   0.       ]], shape=(5, 2), dtype=float32)\n",
      "[[ 0.         1.4285715]\n",
      " [ 2.857143   4.285714 ]\n",
      " [ 5.714286   7.142857 ]\n",
      " [ 8.571428  10.       ]\n",
      " [11.428572  12.857143 ]]\n"
     ]
    }
   ],
   "source": [
    "# 신버전 keras dropout\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "layer = tf.keras.layers.Dropout(0.3, input_shape=(2,)) # 0.3, 0.5 \n",
    "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "print(data)\n",
    "outputs = layer(data, training=True)  # model.fit()호출 시는  학습모드로 training=True가 되어 dropuout 적용\n",
    "                                # model.evaluate() 호출 시는 예측모드로 False가 되어 dropuout이 수행되지 않음\n",
    "print(outputs)\n",
    "print(data * 1/(1 - 0.3)) # 데이터의 30%는 0으로 나머지 데이터는  1/(1-0.3)으로 곱하여 scaled up된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.         0.       ]\n",
      " [ 2.857143   4.285714 ]\n",
      " [ 5.714286   7.1428576]\n",
      " [ 8.571428  10.       ]\n",
      " [11.428572   0.       ]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 구버전 dropout\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "# @tf.function\n",
    "def layer1(X):\n",
    "    return tf.nn.dropout(X, rate=0.3)\n",
    "print(layer1(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
