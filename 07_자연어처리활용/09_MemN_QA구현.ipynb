{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemN으로 질의 응답(Question Answering) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from functools import reduce\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 bAbi 데이터셋\n",
    "# 훈련 데이터 : https://bit.ly/31SqtHy\n",
    "# 테스트 데이터 : https://bit.ly/3f7rH5g\n",
    "\n",
    "# 영어 bAbi(pronounced “baby\") 데이터셋\n",
    "# from tensorflow.keras.utils import get_file\n",
    "# import tarfile\n",
    "\n",
    "# path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/'\n",
    "#                 'babi_tasks_1-20_v1-2.tar.gz')\n",
    "# with tarfile.open(path) as tar:\n",
    "#     tar.extractall()\n",
    "#     tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = os.path.join(\"qa1_single-supporting-fact_train_kor.txt\")\n",
    "TEST_FILE = os.path.join(\"qa1_single-supporting-fact_test_kor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_FILE = os.path.join(\"/content/drive/My Drive/NLP_LAB/qa1_single-supporting-fact_train_kor.txt\")\n",
    "# TEST_FILE = os.path.join(\"/content/drive/My Drive/NLP_LAB/qa1_single-supporting-fact_test_kor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 필웅이는 화장실로 갔습니다.\n",
      "2 은경이는 복도로 이동했습니다.\n",
      "3 필웅이는 어디야? \t화장실\t1\n",
      "4 수종이는 복도로 복귀했습니다.\n",
      "5 경임이는 정원으로 갔습니다.\n",
      "6 수종이는 어디야? \t복도\t4\n",
      "7 은경이는 사무실로 갔습니다.\n",
      "8 경임이는 화장실로 뛰어갔습니다.\n",
      "9 수종이는 어디야? \t복도\t4\n",
      "10 필웅이는 복도로 갔습니다.\n",
      "11 수종이는 사무실로 가버렸습니다.\n",
      "12 수종이는 어디야? \t사무실\t11\n",
      "13 은경이는 정원으로 복귀했습니다.\n",
      "14 은경이는 침실로 갔습니다.\n",
      "15 경임이는 어디야? \t화장실\t8\n",
      "1 경임이는 사무실로 가버렸습니다.\n",
      "2 경임이는 화장실로 이동했습니다.\n",
      "3 경임이는 어디야? \t화장실\t2\n",
      "4 필웅이는 침실로 이동했습니다.\n",
      "5 수종이는 복도로 갔습니다.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "lines = open(TRAIN_FILE , \"rb\")\n",
    "for line in lines:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    i = i + 1\n",
    "    print(line)\n",
    "    if i == 20:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 1부터 15까지 한 개의 스토리이고 그 중간 중간에 질문  \n",
    "# 3번, 6번, 9번, 12번, 15번 라인이 각 스토리 중간에 이어지는 질문, 질문 옆에 정답(정답의 라인번호_)\n",
    "# 숫자 1이 다시 나오면 이제부터는 다시 별개의 스토리가 시작됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir):\n",
    "    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\n",
    "    story_temp = [] # 현재 시점의 스토리 임시 저장\n",
    "    lines = open(dir, \"rb\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.decode(\"utf-8\") # b' 제거\n",
    "        line = line.strip() # '\\n' 제거\n",
    "        idx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\n",
    "        # 여기까지는 모든 줄에 적용되는 전처리\n",
    "\n",
    "        if int(idx) == 1:\n",
    "            story_temp = []\n",
    "\n",
    "        if \"\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
    "            question, answer, _ = text.split(\"\\t\") # 질문과 답변을 각각 저장\n",
    "            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "        else: # 현재 읽는 줄이 스토리인 경우\n",
    "            story_temp.append(text) # 임시 저장\n",
    "\n",
    "    lines.close()\n",
    "    return stories, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = read_data(TRAIN_FILE)\n",
    "test_data = read_data(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n",
    "test_stories, test_questions, test_answers = read_data(TEST_FILE)\n",
    "# train_stories, train_questions, train_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 스토리의 개수 : 10000\n",
      "훈련용 질문의 개수 : 10000\n",
      "훈련용 답변의 개수 : 10000\n",
      "테스트용 스토리의 개수 : 1000\n",
      "테스트용 질문의 개수 : 1000\n",
      "테스트용 답변의 개수 : 1000\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 스토리의 개수 :', len(train_stories))\n",
    "print('훈련용 질문의 개수 :',len(train_questions))\n",
    "print('훈련용 답변의 개수 :',len(train_answers))\n",
    "print('테스트용 스토리의 개수 :',len(test_stories))\n",
    "print('테스트용 질문의 개수 :',len(test_questions))\n",
    "print('테스트용 답변의 개수 :',len(test_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이는 부엌으로 가버렸습니다.',\n",
       " '필웅이는 사무실로 가버렸습니다.',\n",
       " '수종이는 복도로 뛰어갔습니다.',\n",
       " '은경이는 사무실로 복귀했습니다.',\n",
       " '경임이는 사무실로 이동했습니다.',\n",
       " '경임이는 침실로 갔습니다.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories[3572]  # 질문1개 마다 누적 저장되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'은경이는 어디야? '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[3572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사무실'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[3572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf20\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['은', '경이', '는', '화장실', '로', '이동', '했습니다', '.']\n",
      "['경', '임', '이', '는', '정원', '으로', '가버렸습니다', '.']\n",
      "['수종', '이', '는', '복도', '로', '뛰어갔습니다', '.']\n",
      "['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\n",
      "['수종', '이', '는', '사무실', '로', '갔습니다', '.']\n",
      "['은', '경이', '는', '침실', '로', '갔습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# Twitter(Okt) 테스트\n",
    "twitter = Twitter()\n",
    "print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\n",
    "print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\n",
    "print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\n",
    "print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\n",
    "print(twitter.morphs('수종이는 사무실로 갔습니다.'))\n",
    "print(twitter.morphs('은경이는 침실로 갔습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.add_dictionary('은경이', 'Noun')\n",
    "twitter.add_dictionary('경임이', 'Noun')\n",
    "twitter.add_dictionary('수종이', 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['은경이', '는', '화장실', '로', '이동', '했습니다', '.']\n",
      "['경임이', '는', '정원', '으로', '가버렸습니다', '.']\n",
      "['수종이', '는', '복도', '로', '뛰어갔습니다', '.']\n",
      "['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\n",
      "['수종이', '는', '사무실', '로', '갔습니다', '.']\n",
      "['은경이', '는', '침실', '로', '갔습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\n",
    "print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\n",
    "print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\n",
    "print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\n",
    "print(twitter.morphs('수종이는 사무실로 갔습니다.'))\n",
    "print(twitter.morphs('은경이는 침실로 갔습니다.'))\n",
    "# 사전 추가 후에는 이름이 제대로 분리된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return twitter.morphs(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data):\n",
    "    \n",
    "    counter = FreqDist() # 문서에 사용된 단어(토큰)의 사용빈도 정보를 담는 클래스\n",
    "                         # 단어를 키(key), 출현빈도를 값(value)으로 가지는 사전 자료형과 유사\n",
    "        \n",
    "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    # 각 샘플의 길이를 저장하는 리스트\n",
    "    story_len = []\n",
    "    question_len = []\n",
    "\n",
    "    for stories, questions, answers in [train_data, test_data]:\n",
    "        for story in stories:\n",
    "            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
    "            story_len.append(len(stories))     # 각 story의 길이 저장\n",
    "            for word in stories:               # 단어 집합에 단어 추가\n",
    "                counter[word] += 1             # 단어의 출현 빈도를 1 증가\n",
    "        for question in questions:\n",
    "            question = tokenize(question)\n",
    "            question_len.append(len(question))\n",
    "            for word in question:\n",
    "                counter[word] += 1\n",
    "        for answer in answers:\n",
    "            answer = tokenize(answer)\n",
    "            for word in answer:\n",
    "                counter[word] += 1\n",
    "\n",
    "    # 단어 집합 생성\n",
    "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
    "                                                     # most_common(): 가장 출현 횟수가 높은 단어와 빈도를 반환\n",
    "    idx2word = {idx : word for word, idx in word2idx.items()} # key와 value를 서로 바꿈\n",
    "\n",
    "    # 가장 긴 샘플의 길이\n",
    "    story_max_len = np.max(story_len)\n",
    "    question_max_len = np.max(question_len)\n",
    "\n",
    "    return word2idx, idx2word, story_max_len, question_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data) # 시간 소요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'는': 1, '.': 2, '로': 3, '했습니다': 4, '으로': 5, '경임이': 6, '은경이': 7, '수종이': 8, '필웅이': 9, '이동': 10, '가버렸습니다': 11, '뛰어갔습니다': 12, '복귀': 13, '화장실': 14, '정원': 15, '복도': 16, '갔습니다': 17, '사무실': 18, '부엌': 19, '침실': 20, '어디': 21, '야': 22, '?': 23}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스토리의 최대 길이 : 70\n",
      "질문의 최대 길이 : 5\n"
     ]
    }
   ],
   "source": [
    "print('스토리의 최대 길이 :',story_max_len)\n",
    "print('질문의 최대 길이 :',question_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
    "        xq = [word2idx[w] for w in tokenize(question)]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2idx[answer])\n",
    "\n",
    "        # 스토리와 질문은 각각의 최대 길이로 패딩\n",
    "        # 정답은 원-핫 인코딩\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           to_categorical(Y, num_classes=len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n",
    "Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 70) (10000, 5) (10000, 24) (1000, 70) (1000, 5) (1000, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  9,  1, 14,  3, 17,  2,  7,\n",
       "          1, 16,  3, 10,  4,  2],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  9,  1, 14,  3,\n",
       "         17,  2,  7,  1, 16,  3, 10,  4,  2,  8,  1, 16,  3, 13,  4,  2,\n",
       "          6,  1, 15,  5, 17,  2],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          9,  1, 14,  3, 17,  2,  7,  1, 16,  3, 10,  4,  2,  8,  1, 16,\n",
       "          3, 13,  4,  2,  6,  1, 15,  5, 17,  2,  7,  1, 18,  3, 17,  2,\n",
       "          6,  1, 14,  3, 12,  2]]),\n",
       " array([[ 9,  1, 21, 22, 23],\n",
       "        [ 8,  1, 21, 22, 23],\n",
       "        [ 8,  1, 21, 22, 23]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)\n",
    "Xstrain[:3], Xqtrain[:3], Ytrain[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 120   # 에포크 횟수\n",
    "batch_size = 32      # 배치 크기\n",
    "embed_size = 50      # 임베딩 크기\n",
    "lstm_size = 64       # LSTM의 크기\n",
    "dropout_rate = 0.30  # 과적합 방지 기법인 드롭아웃 적용 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stories : Tensor(\"input_1:0\", shape=(None, 70), dtype=float32)\n",
      "Question: Tensor(\"input_2:0\", shape=(None, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_sequence = Input((story_max_len,))\n",
    "question = Input((question_max_len,))\n",
    "\n",
    "print('Stories :', input_sequence)\n",
    "print('Question:', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          1200      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 50)          0         \n",
      "=================================================================\n",
      "Total params: 1,200\n",
      "Trainable params: 1,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 스토리를 위한 첫번째 임베딩. 그림에서 Embedding A\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size))\n",
    "input_encoder_m.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
    "input_encoder_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 5)           120       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 5)           0         \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 스토리를 위한 두번째 임베딩. 그림에서 Embedding C\n",
    "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=question_max_len))\n",
    "input_encoder_c.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)\n",
    "input_encoder_c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5, 50)             1200      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 50)             0         \n",
      "=================================================================\n",
      "Total params: 1,200\n",
      "Trainable params: 1,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 질문을 위한 임베딩. 그림에서 Embedding B\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=embed_size,\n",
    "                               input_length=question_max_len))\n",
    "question_encoder.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원\n",
    "question_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input encoded m Tensor(\"sequential/Identity:0\", shape=(None, 70, 50), dtype=float32)\n",
      "Input encoded c Tensor(\"sequential_1/Identity:0\", shape=(None, 70, 5), dtype=float32)\n",
      "Question encoded Tensor(\"sequential_2/Identity:0\", shape=(None, 5, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 실질적인 임베딩 과정\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "print('Input encoded m', input_encoded_m)\n",
    "print('Input encoded c', input_encoded_c)\n",
    "print('Question encoded', question_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match shape Tensor(\"activation/Identity:0\", shape=(None, 70, 5), dtype=float32)\n",
      "Response shape Tensor(\"permute/Identity:0\", shape=(None, 5, 70), dtype=float32)\n",
      "Answer shape Tensor(\"concatenate/Identity:0\", shape=(None, 5, 120), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
    "# 유사도는 내적을 사용한다.  어텐션 메커니즘의 의도를 갖고 있다\n",
    "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
    "match = Activation('softmax')(match)  # \n",
    "print('Match shape', match)\n",
    "# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\n",
    "response = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\n",
    "print('Response shape', response)\n",
    "\n",
    "# Permute((2, 1)) : 입력 데이터의 차원의 값을  바꿈, (N,70,5) --> (N,5,70)\n",
    "# tf.keras.layers.Permute( dims, **kwargs )  # Permutes the dimensions of the input according to a given pattern.\n",
    "# Permutation pattern does not include the samples dimension. Indexing starts at 1.\n",
    "# For instance, (2, 1) permutes the first and second dimensions of the input.\n",
    "\n",
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    "\n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(dropout_rate)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         multiple             1200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 5, 50)        1200        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 70, 5)        0           sequential[1][0]                 \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 70, 5)        0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             120         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 70, 5)        0           activation[0][0]                 \n",
      "                                                                 sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 5, 70)        0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 120)       0           permute[0][0]                    \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           47360       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 24)           1560        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24)           0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 51,440\n",
      "Trainable params: 51,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 5s 474us/sample - loss: 1.8778 - acc: 0.1776 - val_loss: 1.7605 - val_acc: 0.2020\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 1.6648 - acc: 0.2912 - val_loss: 1.5772 - val_acc: 0.3710\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 1.5471 - acc: 0.3861 - val_loss: 1.4924 - val_acc: 0.4180\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 1.4695 - acc: 0.4303 - val_loss: 1.4197 - val_acc: 0.4720\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 2s 170us/sample - loss: 1.4363 - acc: 0.4570 - val_loss: 1.3855 - val_acc: 0.4720\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 2s 175us/sample - loss: 1.3778 - acc: 0.4764 - val_loss: 1.3247 - val_acc: 0.5090\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 2s 176us/sample - loss: 1.3305 - acc: 0.4973 - val_loss: 1.3114 - val_acc: 0.5060\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 2s 175us/sample - loss: 1.3081 - acc: 0.5018 - val_loss: 1.2754 - val_acc: 0.5180\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 2s 177us/sample - loss: 1.2818 - acc: 0.5053 - val_loss: 1.2615 - val_acc: 0.5230\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 2s 176us/sample - loss: 1.2559 - acc: 0.5116 - val_loss: 1.2393 - val_acc: 0.5320\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 2s 178us/sample - loss: 1.2390 - acc: 0.5208 - val_loss: 1.2398 - val_acc: 0.5140\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 2s 176us/sample - loss: 1.2184 - acc: 0.5197 - val_loss: 1.2243 - val_acc: 0.5230\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 2s 180us/sample - loss: 1.1984 - acc: 0.5210 - val_loss: 1.1941 - val_acc: 0.5310\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 1.1851 - acc: 0.5265 - val_loss: 1.2147 - val_acc: 0.5200\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 2s 178us/sample - loss: 1.1702 - acc: 0.5206 - val_loss: 1.1893 - val_acc: 0.5240\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 2s 181us/sample - loss: 1.1526 - acc: 0.5275 - val_loss: 1.1806 - val_acc: 0.5290\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 2s 179us/sample - loss: 1.1466 - acc: 0.5287 - val_loss: 1.1846 - val_acc: 0.5130\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 2s 182us/sample - loss: 1.1306 - acc: 0.5323 - val_loss: 1.1648 - val_acc: 0.5350\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 2s 193us/sample - loss: 1.1266 - acc: 0.5377 - val_loss: 1.1676 - val_acc: 0.5180\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 2s 187us/sample - loss: 1.1202 - acc: 0.5336 - val_loss: 1.2030 - val_acc: 0.5120\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 2s 180us/sample - loss: 1.1023 - acc: 0.5402 - val_loss: 1.1810 - val_acc: 0.5130\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 2s 179us/sample - loss: 1.1037 - acc: 0.5374 - val_loss: 1.1896 - val_acc: 0.5020\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 2s 181us/sample - loss: 1.0901 - acc: 0.5462 - val_loss: 1.1927 - val_acc: 0.5020\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 2s 182us/sample - loss: 1.0808 - acc: 0.5482 - val_loss: 1.1767 - val_acc: 0.5110\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 2s 192us/sample - loss: 1.0780 - acc: 0.5428 - val_loss: 1.1711 - val_acc: 0.5070\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 2s 179us/sample - loss: 1.0800 - acc: 0.5389 - val_loss: 1.1704 - val_acc: 0.5160\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 2s 184us/sample - loss: 1.0638 - acc: 0.5524 - val_loss: 1.1720 - val_acc: 0.5110\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 1.0563 - acc: 0.5503 - val_loss: 1.1644 - val_acc: 0.5120\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 2s 182us/sample - loss: 1.0585 - acc: 0.5509 - val_loss: 1.1830 - val_acc: 0.5010\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 1.0421 - acc: 0.5595 - val_loss: 1.1909 - val_acc: 0.5050\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 2s 187us/sample - loss: 1.0366 - acc: 0.5572 - val_loss: 1.1956 - val_acc: 0.5040\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 2s 188us/sample - loss: 1.0288 - acc: 0.5563 - val_loss: 1.1949 - val_acc: 0.5010\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 1.0171 - acc: 0.5658 - val_loss: 1.1849 - val_acc: 0.5060\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 2s 188us/sample - loss: 1.0193 - acc: 0.5640 - val_loss: 1.2096 - val_acc: 0.5020\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 2s 188us/sample - loss: 1.0130 - acc: 0.5597 - val_loss: 1.2057 - val_acc: 0.5030\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 2s 186us/sample - loss: 1.0031 - acc: 0.5740 - val_loss: 1.2102 - val_acc: 0.5070\n",
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 2s 190us/sample - loss: 0.9950 - acc: 0.5786 - val_loss: 1.2089 - val_acc: 0.5120\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 0.9835 - acc: 0.5742 - val_loss: 1.2209 - val_acc: 0.5090\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 2s 188us/sample - loss: 0.9771 - acc: 0.5860 - val_loss: 1.2208 - val_acc: 0.5190\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 2s 190us/sample - loss: 0.9750 - acc: 0.5847 - val_loss: 1.2209 - val_acc: 0.4910\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 2s 188us/sample - loss: 0.9697 - acc: 0.5901 - val_loss: 1.2245 - val_acc: 0.4960\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 0.9570 - acc: 0.5950 - val_loss: 1.2339 - val_acc: 0.5010\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 2s 191us/sample - loss: 0.9605 - acc: 0.5916 - val_loss: 1.2427 - val_acc: 0.4910\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 0.9443 - acc: 0.5968 - val_loss: 1.2341 - val_acc: 0.4960\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 2s 190us/sample - loss: 0.9347 - acc: 0.6044 - val_loss: 1.2419 - val_acc: 0.4980\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 2s 194us/sample - loss: 0.9278 - acc: 0.6126 - val_loss: 1.2350 - val_acc: 0.5090\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.9149 - acc: 0.6159 - val_loss: 1.2385 - val_acc: 0.5270\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 2s 191us/sample - loss: 0.8852 - acc: 0.6371 - val_loss: 1.2066 - val_acc: 0.5300\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.8310 - acc: 0.6704 - val_loss: 1.0990 - val_acc: 0.6200\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 2s 195us/sample - loss: 0.7091 - acc: 0.7338 - val_loss: 0.8882 - val_acc: 0.6850\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 2s 193us/sample - loss: 0.6092 - acc: 0.7780 - val_loss: 0.7824 - val_acc: 0.7180\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 2s 190us/sample - loss: 0.5534 - acc: 0.7987 - val_loss: 0.7484 - val_acc: 0.7380\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.5129 - acc: 0.8113 - val_loss: 0.7336 - val_acc: 0.7370\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.4827 - acc: 0.8214 - val_loss: 0.6348 - val_acc: 0.7700\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 2s 196us/sample - loss: 0.4327 - acc: 0.8394 - val_loss: 0.6020 - val_acc: 0.7860\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 2s 196us/sample - loss: 0.3972 - acc: 0.8547 - val_loss: 0.5508 - val_acc: 0.8090\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 2s 196us/sample - loss: 0.3650 - acc: 0.8637 - val_loss: 0.5365 - val_acc: 0.8250\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 2s 194us/sample - loss: 0.3316 - acc: 0.8799 - val_loss: 0.4879 - val_acc: 0.8330\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 2s 196us/sample - loss: 0.3072 - acc: 0.8895 - val_loss: 0.4703 - val_acc: 0.8440\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 2s 197us/sample - loss: 0.2965 - acc: 0.8931 - val_loss: 0.4451 - val_acc: 0.8480\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 2s 197us/sample - loss: 0.2774 - acc: 0.8980 - val_loss: 0.4487 - val_acc: 0.8510\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 2s 195us/sample - loss: 0.2577 - acc: 0.9055 - val_loss: 0.4311 - val_acc: 0.8530\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 2s 198us/sample - loss: 0.2422 - acc: 0.9116 - val_loss: 0.4609 - val_acc: 0.8560\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 2s 195us/sample - loss: 0.2294 - acc: 0.9194 - val_loss: 0.4421 - val_acc: 0.8660\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 2s 195us/sample - loss: 0.2189 - acc: 0.9196 - val_loss: 0.4202 - val_acc: 0.8690\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 2s 198us/sample - loss: 0.2062 - acc: 0.9270 - val_loss: 0.4293 - val_acc: 0.8710\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 2s 196us/sample - loss: 0.1877 - acc: 0.9339 - val_loss: 0.3886 - val_acc: 0.8770\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 2s 199us/sample - loss: 0.1737 - acc: 0.9358 - val_loss: 0.3740 - val_acc: 0.8850\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 2s 199us/sample - loss: 0.1648 - acc: 0.9417 - val_loss: 0.3801 - val_acc: 0.8830\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 2s 197us/sample - loss: 0.1542 - acc: 0.9471 - val_loss: 0.3368 - val_acc: 0.8990\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 2s 198us/sample - loss: 0.1441 - acc: 0.9501 - val_loss: 0.3537 - val_acc: 0.8950\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 2s 200us/sample - loss: 0.1283 - acc: 0.9546 - val_loss: 0.3128 - val_acc: 0.9000\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 2s 199us/sample - loss: 0.1164 - acc: 0.9593 - val_loss: 0.3180 - val_acc: 0.9100\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 2s 198us/sample - loss: 0.1113 - acc: 0.9633 - val_loss: 0.3171 - val_acc: 0.9020\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 2s 200us/sample - loss: 0.1011 - acc: 0.9648 - val_loss: 0.2780 - val_acc: 0.9230\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 2s 201us/sample - loss: 0.1031 - acc: 0.9646 - val_loss: 0.2982 - val_acc: 0.9180\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 2s 203us/sample - loss: 0.0946 - acc: 0.9682 - val_loss: 0.2510 - val_acc: 0.9220\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 2s 205us/sample - loss: 0.0937 - acc: 0.9693 - val_loss: 0.2939 - val_acc: 0.9140\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 2s 201us/sample - loss: 0.0826 - acc: 0.9706 - val_loss: 0.3282 - val_acc: 0.9190\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 2s 202us/sample - loss: 0.0811 - acc: 0.9745 - val_loss: 0.2491 - val_acc: 0.9240\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 2s 205us/sample - loss: 0.0762 - acc: 0.9763 - val_loss: 0.2640 - val_acc: 0.9260\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 2s 230us/sample - loss: 0.0734 - acc: 0.9769 - val_loss: 0.2565 - val_acc: 0.9250\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 2s 204us/sample - loss: 0.0626 - acc: 0.9790 - val_loss: 0.2580 - val_acc: 0.9340\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 2s 201us/sample - loss: 0.0639 - acc: 0.9778 - val_loss: 0.2386 - val_acc: 0.9320\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 2s 207us/sample - loss: 0.0628 - acc: 0.9791 - val_loss: 0.2802 - val_acc: 0.9260\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 2s 206us/sample - loss: 0.0566 - acc: 0.9818 - val_loss: 0.2584 - val_acc: 0.9340\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 2s 208us/sample - loss: 0.0537 - acc: 0.9817 - val_loss: 0.2448 - val_acc: 0.9380\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 2s 210us/sample - loss: 0.0569 - acc: 0.9812 - val_loss: 0.2879 - val_acc: 0.9360\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 2s 210us/sample - loss: 0.0539 - acc: 0.9818 - val_loss: 0.2287 - val_acc: 0.9370\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 2s 210us/sample - loss: 0.0486 - acc: 0.9854 - val_loss: 0.2304 - val_acc: 0.9370\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 0.0449 - acc: 0.9850 - val_loss: 0.2897 - val_acc: 0.9320\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.0509 - acc: 0.9830 - val_loss: 0.2523 - val_acc: 0.9380\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 2s 207us/sample - loss: 0.0520 - acc: 0.9838 - val_loss: 0.2608 - val_acc: 0.9370\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.0421 - acc: 0.9859 - val_loss: 0.2625 - val_acc: 0.9380\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 2s 218us/sample - loss: 0.0377 - acc: 0.9872 - val_loss: 0.2598 - val_acc: 0.9410\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 2s 210us/sample - loss: 0.0415 - acc: 0.9881 - val_loss: 0.2838 - val_acc: 0.9320\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0419 - acc: 0.9881 - val_loss: 0.2746 - val_acc: 0.9360\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 0.0391 - acc: 0.9873 - val_loss: 0.2389 - val_acc: 0.9410\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 2s 234us/sample - loss: 0.0383 - acc: 0.9875 - val_loss: 0.2767 - val_acc: 0.9380\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 0.0336 - acc: 0.9874 - val_loss: 0.3256 - val_acc: 0.9400\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 0.0342 - acc: 0.9895 - val_loss: 0.2716 - val_acc: 0.9430\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 2s 212us/sample - loss: 0.0325 - acc: 0.9903 - val_loss: 0.2925 - val_acc: 0.9380\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 2s 208us/sample - loss: 0.0310 - acc: 0.9896 - val_loss: 0.2733 - val_acc: 0.9370\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 0.0352 - acc: 0.9894 - val_loss: 0.2838 - val_acc: 0.9420\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 0.0353 - acc: 0.9892 - val_loss: 0.2650 - val_acc: 0.9430\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.0280 - acc: 0.9913 - val_loss: 0.2964 - val_acc: 0.9420\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 0.0288 - acc: 0.9911 - val_loss: 0.3041 - val_acc: 0.9440\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.0364 - acc: 0.9883 - val_loss: 0.2676 - val_acc: 0.9500\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 0.0301 - acc: 0.9909 - val_loss: 0.2860 - val_acc: 0.9440\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 2s 230us/sample - loss: 0.0273 - acc: 0.9919 - val_loss: 0.2785 - val_acc: 0.9470\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 2s 220us/sample - loss: 0.0265 - acc: 0.9920 - val_loss: 0.2735 - val_acc: 0.9460\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 0.0311 - acc: 0.9914 - val_loss: 0.3039 - val_acc: 0.9400\n",
      "Epoch 113/120\n",
      "10000/10000 [==============================] - 2s 216us/sample - loss: 0.0279 - acc: 0.9926 - val_loss: 0.2969 - val_acc: 0.9430\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 0.0272 - acc: 0.9911 - val_loss: 0.2833 - val_acc: 0.9430\n",
      "Epoch 115/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 231us/sample - loss: 0.0241 - acc: 0.9928 - val_loss: 0.2970 - val_acc: 0.9470\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 2s 225us/sample - loss: 0.0249 - acc: 0.9935 - val_loss: 0.2737 - val_acc: 0.9460\n",
      "Epoch 117/120\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0253 - acc: 0.9912 - val_loss: 0.2992 - val_acc: 0.9420\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 2s 234us/sample - loss: 0.0292 - acc: 0.9923 - val_loss: 0.2891 - val_acc: 0.9400\n",
      "Epoch 119/120\n",
      "10000/10000 [==============================] - 2s 229us/sample - loss: 0.0349 - acc: 0.9905 - val_loss: 0.2698 - val_acc: 0.9440\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 0.0262 - acc: 0.9934 - val_loss: 0.3009 - val_acc: 0.9410\n"
     ]
    }
   ],
   "source": [
    "# start training the model, 약 4분 소요(CPU)\n",
    "history = model.fit([Xstrain, Xqtrain],\n",
    "         Ytrain, batch_size, train_epochs,\n",
    "         validation_data=([Xstest, Xqtest], Ytest))\n",
    "\n",
    "# save model\n",
    "model.save('model.h5') # 모델의 구조, 가중치, 훈련 설정을 하나의 파일에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# labels\n",
    "ytest = np.argmax(Ytest, axis=1)\n",
    "\n",
    "# get predictions\n",
    "Ytest_ = model.predict([Xstest, Xqtest])\n",
    "ytest_ = np.argmax(Ytest_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# new_model = tf.keras.models.load_model('model.h5') # 저장된 모델로부터 새로운 케라스 모델을 로드\n",
    "# new_model.summary() # 저장했던 모델과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문                |실제값  |예측값\n",
      "---------------------------------------\n",
      "은경이 는 어디 야 ?        : 복도      복도\n",
      "필웅이 는 어디 야 ?        : 화장실     화장실\n",
      "경임이 는 어디 야 ?        : 부엌      부엌\n",
      "경임이 는 어디 야 ?        : 복도      복도\n",
      "경임이 는 어디 야 ?        : 부엌      부엌\n",
      "경임이 는 어디 야 ?        : 복도      복도\n",
      "경임이 는 어디 야 ?        : 정원      정원\n",
      "수종이 는 어디 야 ?        : 복도      복도\n",
      "경임이 는 어디 야 ?        : 사무실     사무실\n",
      "수종이 는 어디 야 ?        : 사무실     복도\n",
      "필웅이 는 어디 야 ?        : 부엌      부엌\n",
      "필웅이 는 어디 야 ?        : 정원      정원\n",
      "수종이 는 어디 야 ?        : 사무실     사무실\n",
      "필웅이 는 어디 야 ?        : 침실      침실\n",
      "필웅이 는 어디 야 ?        : 침실      침실\n",
      "은경이 는 어디 야 ?        : 부엌      부엌\n",
      "은경이 는 어디 야 ?        : 정원      정원\n",
      "은경이 는 어디 야 ?        : 부엌      부엌\n",
      "수종이 는 어디 야 ?        : 사무실     부엌\n",
      "은경이 는 어디 야 ?        : 부엌      정원\n",
      "필웅이 는 어디 야 ?        : 복도      복도\n",
      "은경이 는 어디 야 ?        : 사무실     사무실\n",
      "은경이 는 어디 야 ?        : 사무실     사무실\n",
      "경임이 는 어디 야 ?        : 복도      복도\n",
      "수종이 는 어디 야 ?        : 침실      침실\n",
      "경임이 는 어디 야 ?        : 침실      침실\n",
      "필웅이 는 어디 야 ?        : 침실      침실\n",
      "수종이 는 어디 야 ?        : 부엌      부엌\n",
      "수종이 는 어디 야 ?        : 부엌      부엌\n",
      "수종이 는 어디 야 ?        : 부엌      부엌\n"
     ]
    }
   ],
   "source": [
    "NUM_DISPLAY = 30\n",
    "\n",
    "print(\"{:18}|{:5}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\n",
    "print(39 * \"-\")\n",
    "\n",
    "for i in range(NUM_DISPLAY):\n",
    "    question = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\n",
    "    label = idx2word[ytest[i]]\n",
    "    prediction = idx2word[ytest_[i]]\n",
    "    print(\"{:20}: {:7} {}\".format(question, label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
